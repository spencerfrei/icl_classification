{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating checkpoints/checkpoint_d500_B500_R111_20241118_163035_step_499.pt\n",
      "\n",
      "Evaluating risk curve for d=500, label_flip_p=0.0\n",
      "Position 10: accuracy = 1.000\n",
      "Position 20: accuracy = 1.000\n",
      "Position 30: accuracy = 1.000\n",
      "Position 40: accuracy = 1.000\n",
      "Position 50: accuracy = 1.000\n",
      "Position 60: accuracy = 1.000\n",
      "Position 70: accuracy = 1.000\n",
      "Position 80: accuracy = 1.000\n",
      "Position 90: accuracy = 1.000\n",
      "Position 100: accuracy = 1.000\n",
      "Position 110: accuracy = 1.000\n",
      "Position 120: accuracy = 1.000\n",
      "Position 130: accuracy = 1.000\n",
      "Position 140: accuracy = 1.000\n",
      "Position 150: accuracy = 1.000\n",
      "Position 160: accuracy = 1.000\n",
      "Position 170: accuracy = 1.000\n",
      "Position 180: accuracy = 1.000\n",
      "Position 190: accuracy = 1.000\n",
      "Position 200: accuracy = 1.000\n",
      "Position 210: accuracy = 1.000\n",
      "Position 220: accuracy = 1.000\n",
      "Position 230: accuracy = 1.000\n",
      "Position 240: accuracy = 1.000\n",
      "Position 250: accuracy = 1.000\n",
      "Position 260: accuracy = 1.000\n",
      "Position 270: accuracy = 1.000\n",
      "Position 280: accuracy = 1.000\n",
      "Position 290: accuracy = 1.000\n",
      "Position 300: accuracy = 1.000\n",
      "Position 310: accuracy = 1.000\n",
      "Position 320: accuracy = 1.000\n",
      "Position 330: accuracy = 1.000\n",
      "Position 340: accuracy = 1.000\n",
      "Position 350: accuracy = 1.000\n",
      "Position 360: accuracy = 1.000\n",
      "Position 370: accuracy = 1.000\n",
      "Position 380: accuracy = 1.000\n",
      "Position 390: accuracy = 1.000\n",
      "Position 400: accuracy = 1.000\n",
      "Position 410: accuracy = 1.000\n",
      "Position 420: accuracy = 1.000\n",
      "Position 430: accuracy = 1.000\n",
      "Position 440: accuracy = 1.000\n",
      "Position 450: accuracy = 1.000\n",
      "Position 460: accuracy = 1.000\n",
      "Position 470: accuracy = 1.000\n"
     ]
    }
   ],
   "source": [
    "from classification_icl import ExperimentConfig, LinearTransformer, GaussianMixtureDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import asdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "class CheckpointEvaluator:\n",
    "    \"\"\"Evaluator class for analyzing trained model checkpoints\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir: str):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        \n",
    "    def load_checkpoint(self, checkpoint_path: str) -> Tuple[LinearTransformer, ExperimentConfig]:\n",
    "        \"\"\"Load model and config from checkpoint\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        config = ExperimentConfig(**asdict(checkpoint['config']))\n",
    "        model = LinearTransformer(config.d)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        return model, config\n",
    "\n",
    "    def evaluate_risk_curves(\n",
    "        self,\n",
    "        model: LinearTransformer,\n",
    "        d: int,\n",
    "        max_seq_length: int = 50,\n",
    "        num_samples: int = 1000,\n",
    "        label_flip_ps: List[float] = [0.0, 0.15],\n",
    "        device: str = 'cpu'\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Evaluate model's risk curve - accuracy vs number of context examples.\n",
    "        \"\"\"\n",
    "        model = model.to(device)\n",
    "        model.W.data = torch.eye(d, device=device)\n",
    "        results = {}\n",
    "        R = d ** 0.7\n",
    "        \n",
    "        for label_flip_p in label_flip_ps:\n",
    "            print(f\"\\nEvaluating risk curve for d={d}, label_flip_p={label_flip_p}\")\n",
    "            \n",
    "            # Create dataset with reasonable size N\n",
    "            dataset = GaussianMixtureDataset(\n",
    "                d=d,\n",
    "                N=max_seq_length,\n",
    "                B=num_samples,\n",
    "                R=R,  # Using R=1.0 like in test\n",
    "                is_validation=True,\n",
    "                label_flip_p=label_flip_p\n",
    "            )\n",
    "            \n",
    "            # Get all data at once\n",
    "            context_x, context_y, _, _ = [t.to(device) for t in dataset[0]]\n",
    "            \n",
    "            # Storage for accuracies at each position\n",
    "            position_accuracies = np.zeros(max_seq_length-1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for k in range(1, max_seq_length):\n",
    "                    curr_context_x = context_x[:, :k]\n",
    "                    curr_context_y = context_y[:, :k]\n",
    "                    \n",
    "                    preds = model.compute_in_context_preds(curr_context_x, curr_context_y)\n",
    "                    accuracy = (preds == curr_context_y).float().mean().item()\n",
    "                    position_accuracies[k-1] = accuracy\n",
    "                    \n",
    "                    if k % 10 == 0:\n",
    "                        print(f\"Position {k}: accuracy = {accuracy:.3f}\")\n",
    "            \n",
    "            results[label_flip_p] = position_accuracies\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def old_evaluate_risk_curves(\n",
    "        self,\n",
    "        model: LinearTransformer,\n",
    "        d: int,\n",
    "        max_seq_length: int = 50,\n",
    "        num_samples: int = 1000,\n",
    "        label_flip_ps: List[float] = [0.0, 0.15],\n",
    "        device: str = 'cpu'\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Evaluate model's risk curve - accuracy vs number of context examples.\n",
    "        Uses the model's compute_in_context_preds method for efficiency.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained LinearTransformer model\n",
    "            d: Input dimension\n",
    "            max_seq_length: Maximum sequence length to evaluate\n",
    "            num_samples: Number of sequences to evaluate\n",
    "            label_flip_ps: List of label flip probabilities to evaluate\n",
    "            device: Device to run evaluation on\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping label_flip_p to arrays of accuracies at each position\n",
    "        \"\"\"\n",
    "        model = model.to(device)\n",
    "        model.W.data = torch.eye(d, device=device) #TODO remove shortly, just testing for now\n",
    "        results = {}\n",
    "        R = 1\n",
    "        \n",
    "        for label_flip_p in label_flip_ps:\n",
    "            print(f\"\\nEvaluating risk curve for d={d}, label_flip_p={label_flip_p}\")\n",
    "            \n",
    "            # Create dataset with longer sequences\n",
    "            dataset = GaussianMixtureDataset(\n",
    "                d=d,\n",
    "                N=max_seq_length,\n",
    "                B=num_samples,\n",
    "                R=R,\n",
    "                is_validation=True,\n",
    "                label_flip_p=label_flip_p\n",
    "            )\n",
    "            \n",
    "            # Get all data at once\n",
    "            context_x, context_y, _, _ = [t.to(device) for t in dataset[0]]\n",
    "            \n",
    "            # Storage for accuracies at each position\n",
    "            position_accuracies = np.zeros(max_seq_length-1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for k in range(1, max_seq_length):\n",
    "\n",
    "                    # Use k examples to predict k+1\n",
    "                    curr_context_x = context_x[:, :k]\n",
    "                    curr_context_y = context_y[:, :k]\n",
    "                    # next_y = context_y[:, k]\n",
    "                    \n",
    "                    # Use model's built-in prediction method for the k+1 position\n",
    "                    preds = model.compute_in_context_preds(curr_context_x, curr_context_y)\n",
    "                    accuracy = (preds == curr_context_y).float().mean().item()\n",
    "                    position_accuracies[k-1] = accuracy\n",
    "                    \n",
    "                    if k % 10 == 0:\n",
    "                        print(f\"Position {k}: accuracy = {accuracy:.3f}\")\n",
    "            \n",
    "            results[label_flip_p] = position_accuracies\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def plot_risk_curves(self, results_by_d: Dict[int, Dict[float, np.ndarray]], save_path: str = None):\n",
    "        \"\"\"\n",
    "        Plot risk curves for different dimensions and label flip probabilities.\n",
    "        \n",
    "        Args:\n",
    "            results_by_d: Dictionary mapping dimension d to results from evaluate_risk_curves\n",
    "            save_path: Optional path to save the plot\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        colors = ['blue', 'red', 'green']\n",
    "        styles = ['-', '--']\n",
    "        \n",
    "        for i, (d, results) in enumerate(sorted(results_by_d.items())):\n",
    "            for j, (label_flip_p, accuracies) in enumerate(results.items()):\n",
    "                x = np.arange(1, len(accuracies) + 1)\n",
    "                label = f'd={d}, p={label_flip_p}'\n",
    "                plt.plot(x, accuracies, label=label, color=colors[i], linestyle=styles[j])\n",
    "        \n",
    "        plt.xlabel('Number of Context Examples (k)')\n",
    "        plt.ylabel('Memorization accuracy')\n",
    "        plt.title('Memorization accuracy vs. number of in-context examples')\n",
    "        #plt.ylabel('Accuracy on (k+1)th Example')\n",
    "        #plt.title('Risk Curves for Different Dimensions and Label Noise')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            print(f\"Plot saved to {save_path}\")\n",
    "        else:\n",
    "            plt.show()\n",
    "            \n",
    "    def evaluate_checkpoint(self, checkpoint_file: str) -> Dict[int, Dict[float, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Evaluate a single checkpoint and return risk curves.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_file: Path to checkpoint file\n",
    "            \n",
    "        Returns:\n",
    "            Results dictionary for plotting risk curves\n",
    "        \"\"\"\n",
    "        print(f\"\\nEvaluating {checkpoint_file}\")\n",
    "        model, config = self.load_checkpoint(checkpoint_file)\n",
    "        results = self.evaluate_risk_curves(\n",
    "            model=model,\n",
    "            d=config.d,\n",
    "            max_seq_length=600,\n",
    "            num_samples=1000,\n",
    "            label_flip_ps=[0.0, 0.15],\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "        return {config.d: results}\n",
    "\n",
    "# Example usage:\n",
    "def main():\n",
    "    evaluator = CheckpointEvaluator('checkpoints/')\n",
    "    \n",
    "    # Collect results for all dimensions\n",
    "    all_results = {}\n",
    "    # dimensions = [50, 500, 2500]\n",
    "    dimensions = [500]\n",
    "    \n",
    "    for d in dimensions:\n",
    "        # checkpoint_file = f\"checkpoints/checkpoint_d{d}_*.pt\"  # Adjust pattern as needed\n",
    "        matches = list(Path('checkpoints/').glob(f\"checkpoint_d{d}*.pt\"))\n",
    "        if matches:\n",
    "            results = evaluator.evaluate_checkpoint(str(matches[0]))\n",
    "            all_results.update(results)\n",
    "    \n",
    "    # Plot combined results\n",
    "    evaluator.plot_risk_curves(all_results, save_path=None)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing R = 1.0 ===\n",
      "\n",
      "Sequence length k = 1\n",
      "X norms - mean: 31.72, std: 0.77\n",
      "Contribution norms - mean: 31.72, std: 0.77\n",
      "hat_mu norms - mean: 31.72, std: 0.77\n",
      "Logit magnitudes - mean: 1006.67, std: 999.79\n",
      "Logit range: [-1116.14, 1096.52]\n",
      "Accuracy: 1.000\n",
      "\n",
      "Sequence length k = 5\n",
      "X norms - mean: 31.63, std: 0.70\n",
      "Contribution norms - mean: 31.63, std: 0.70\n",
      "hat_mu norms - mean: 14.16, std: 0.31\n",
      "Logit magnitudes - mean: 200.50, std: 201.29\n",
      "Logit range: [-240.88, 234.39]\n",
      "Accuracy: 1.000\n",
      "\n",
      "Sequence length k = 10\n",
      "X norms - mean: 31.63, std: 0.70\n",
      "Contribution norms - mean: 31.63, std: 0.70\n",
      "hat_mu norms - mean: 9.98, std: 0.20\n",
      "Logit magnitudes - mean: 99.70, std: 100.29\n",
      "Logit range: [-130.24, 127.40]\n",
      "Accuracy: 1.000\n",
      "\n",
      "Sequence length k = 20\n",
      "X norms - mean: 31.62, std: 0.70\n",
      "Contribution norms - mean: 31.62, std: 0.70\n",
      "hat_mu norms - mean: 7.09, std: 0.14\n",
      "Logit magnitudes - mean: 50.27, std: 50.79\n",
      "Logit range: [-77.50, 74.13]\n",
      "Accuracy: 1.000\n",
      "\n",
      "Sequence length k = 50\n",
      "X norms - mean: 31.63, std: 0.70\n",
      "Contribution norms - mean: 31.63, std: 0.70\n",
      "hat_mu norms - mean: 4.53, std: 0.10\n",
      "Logit magnitudes - mean: 20.50, std: 21.01\n",
      "Logit range: [-38.08, 35.20]\n",
      "Accuracy: 1.000\n",
      "\n",
      "=== Testing R = 1000 ===\n",
      "\n",
      "Sequence length k = 1\n",
      "X norms - mean: 1000.47, std: 1.08\n",
      "Contribution norms - mean: 1000.47, std: 1.08\n",
      "hat_mu norms - mean: 1000.47, std: 1.08\n",
      "Logit magnitudes - mean: 1000946.31, std: 993054.69\n",
      "Logit range: [-1005212.94, 1006144.94]\n",
      "Accuracy: 1.000\n",
      "\n",
      "Sequence length k = 5\n",
      "X norms - mean: 1000.44, std: 1.04\n",
      "Contribution norms - mean: 1000.44, std: 1.04\n",
      "hat_mu norms - mean: 716.16, std: 285.84\n",
      "Logit magnitudes - mean: 716094.44, std: 770725.19\n",
      "Logit range: [-1004503.69, 1002934.50]\n",
      "Accuracy: 0.858\n",
      "\n",
      "Example of failed reconstruction:\n",
      "Original y: [1. 1. 1. 1. 0.]\n",
      "Predicted:  [0. 0. 1. 1. 0.]\n",
      "Logits:     [-199716.25 -199801.5   200160.19  200082.08 -200197.47]\n",
      "Position 0: y_i * (x_i @ x_i) = 1000306.75000\n",
      "Position 1: y_i * (x_i @ x_i) = 1001191.31250\n",
      "Position 2: y_i * (x_i @ x_i) = 1000793.50000\n",
      "Position 3: y_i * (x_i @ x_i) = 1000149.00000\n",
      "Position 4: y_i * (x_i @ x_i) = -1001250.87500\n",
      "\n",
      "Sequence length k = 10\n",
      "X norms - mean: 1000.49, std: 1.01\n",
      "Contribution norms - mean: 1000.49, std: 1.01\n",
      "hat_mu norms - mean: 692.09, std: 215.40\n",
      "Logit magnitudes - mean: 692073.00, std: 724880.38\n",
      "Logit range: [-1002960.12, 1002732.25]\n",
      "Accuracy: 0.846\n",
      "\n",
      "Example of failed reconstruction:\n",
      "Original y: [1. 1. 1. 1. 0. 1. 0. 1. 0. 1.]\n",
      "Predicted:  [0. 0. 1. 1. 0. 1. 0. 1. 0. 0.]\n",
      "Logits:     [-399833.78 -399994.84  400137.56  399997.03 -400218.75  400064.5\n",
      " -400179.78  400528.97 -400338.5  -400067.2 ]\n",
      "Position 0: y_i * (x_i @ x_i) = 1000306.75000\n",
      "Position 1: y_i * (x_i @ x_i) = 1001191.31250\n",
      "Position 2: y_i * (x_i @ x_i) = 1000793.50000\n",
      "Position 3: y_i * (x_i @ x_i) = 1000149.00000\n",
      "Position 4: y_i * (x_i @ x_i) = -1001250.87500\n",
      "Position 5: y_i * (x_i @ x_i) = 1000562.25000\n",
      "Position 6: y_i * (x_i @ x_i) = -1001003.37500\n",
      "Position 7: y_i * (x_i @ x_i) = 1002658.75000\n",
      "Position 8: y_i * (x_i @ x_i) = -1001805.43750\n",
      "Position 9: y_i * (x_i @ x_i) = 1001392.31250\n",
      "\n",
      "Sequence length k = 20\n",
      "X norms - mean: 1000.48, std: 1.02\n",
      "Contribution norms - mean: 1000.48, std: 1.02\n",
      "hat_mu norms - mean: 685.02, std: 141.71\n",
      "Logit magnitudes - mean: 685001.62, std: 699541.12\n",
      "Logit range: [-1001416.69, 1002448.56]\n",
      "Accuracy: 0.843\n",
      "\n",
      "Example of failed reconstruction:\n",
      "Original y: [1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1.]\n",
      "Predicted:  [0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0.]\n",
      "Logits:     [-499797.03 -500007.6   500023.97  499856.44 -500130.84  499938.06\n",
      " -500077.12  500512.8  -500257.84 -500084.06  499864.6   499162.34\n",
      "  499763.   -500432.6   500569.    500361.84 -500599.84  499838.4\n",
      " -499595.3  -500624.8 ]\n",
      "Position 0: y_i * (x_i @ x_i) = 1000306.75000\n",
      "Position 1: y_i * (x_i @ x_i) = 1001191.31250\n",
      "Position 2: y_i * (x_i @ x_i) = 1000793.50000\n",
      "Position 3: y_i * (x_i @ x_i) = 1000149.00000\n",
      "Position 4: y_i * (x_i @ x_i) = -1001250.87500\n",
      "Position 5: y_i * (x_i @ x_i) = 1000562.25000\n",
      "Position 6: y_i * (x_i @ x_i) = -1001003.37500\n",
      "Position 7: y_i * (x_i @ x_i) = 1002658.75000\n",
      "Position 8: y_i * (x_i @ x_i) = -1001805.43750\n",
      "Position 9: y_i * (x_i @ x_i) = 1001392.31250\n",
      "Position 10: y_i * (x_i @ x_i) = 1000218.50000\n",
      "Position 11: y_i * (x_i @ x_i) = 997394.62500\n",
      "Position 12: y_i * (x_i @ x_i) = 999698.12500\n",
      "Position 13: y_i * (x_i @ x_i) = -1002423.25000\n",
      "Position 14: y_i * (x_i @ x_i) = 1003022.62500\n",
      "Position 15: y_i * (x_i @ x_i) = 1002203.50000\n",
      "Position 16: y_i * (x_i @ x_i) = -1003146.00000\n",
      "Position 17: y_i * (x_i @ x_i) = 1000091.87500\n",
      "Position 18: y_i * (x_i @ x_i) = 999513.75000\n",
      "Position 19: y_i * (x_i @ x_i) = 1003598.62500\n",
      "\n",
      "Sequence length k = 50\n",
      "X norms - mean: 1000.50, std: 1.01\n",
      "Contribution norms - mean: 1000.50, std: 1.01\n",
      "hat_mu norms - mean: 689.61, std: 96.67\n",
      "Logit magnitudes - mean: 689612.94, std: 696354.88\n",
      "Logit range: [-921442.94, 921245.38]\n",
      "Accuracy: 0.845\n",
      "\n",
      "Example of failed reconstruction:\n",
      "Original y: [1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0.]\n",
      "Predicted:  [0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Logits:     [-639718.1  -639981.    639921.56  639722.9  -640064.5   639813.94\n",
      " -639986.1   640544.1  -640211.75 -640079.    639723.5   638816.4\n",
      "  639587.1  -640432.06  640619.75  640355.2  -640651.94  639692.\n",
      " -639458.94 -640771.25  640612.75  639069.4  -639641.44 -640201.6\n",
      " -639679.6   640130.   -639749.25  640377.44 -640179.6   640681.3\n",
      " -640050.9  -638737.4   639888.6  -640412.7  -638887.1  -640222.4\n",
      "  640338.1   638934.06 -640004.    641001.1  -640111.6   640281.2\n",
      "  639221.5   640082.56  639206.8  -639863.4  -640755.   -640163.\n",
      " -640194.5  -640129.7 ]\n",
      "Position 0: y_i * (x_i @ x_i) = 1000306.75000\n",
      "Position 1: y_i * (x_i @ x_i) = 1001191.31250\n",
      "Position 2: y_i * (x_i @ x_i) = 1000793.50000\n",
      "Position 3: y_i * (x_i @ x_i) = 1000149.00000\n",
      "Position 4: y_i * (x_i @ x_i) = -1001250.87500\n",
      "Position 5: y_i * (x_i @ x_i) = 1000562.25000\n",
      "Position 6: y_i * (x_i @ x_i) = -1001003.37500\n",
      "Position 7: y_i * (x_i @ x_i) = 1002658.75000\n",
      "Position 8: y_i * (x_i @ x_i) = -1001805.43750\n",
      "Position 9: y_i * (x_i @ x_i) = 1001392.31250\n",
      "Position 10: y_i * (x_i @ x_i) = 1000218.50000\n",
      "Position 11: y_i * (x_i @ x_i) = 997394.62500\n",
      "Position 12: y_i * (x_i @ x_i) = 999698.12500\n",
      "Position 13: y_i * (x_i @ x_i) = -1002423.25000\n",
      "Position 14: y_i * (x_i @ x_i) = 1003022.62500\n",
      "Position 15: y_i * (x_i @ x_i) = 1002203.50000\n",
      "Position 16: y_i * (x_i @ x_i) = -1003146.00000\n",
      "Position 17: y_i * (x_i @ x_i) = 1000091.87500\n",
      "Position 18: y_i * (x_i @ x_i) = 999513.75000\n",
      "Position 19: y_i * (x_i @ x_i) = 1003598.62500\n",
      "Position 20: y_i * (x_i @ x_i) = 1003009.87500\n",
      "Position 21: y_i * (x_i @ x_i) = 998240.87500\n",
      "Position 22: y_i * (x_i @ x_i) = -999992.50000\n",
      "Position 23: y_i * (x_i @ x_i) = -1001660.93750\n",
      "Position 24: y_i * (x_i @ x_i) = -1000046.00000\n",
      "Position 25: y_i * (x_i @ x_i) = 1001467.87500\n",
      "Position 26: y_i * (x_i @ x_i) = -1000264.12500\n",
      "Position 27: y_i * (x_i @ x_i) = 1002312.37500\n",
      "Position 28: y_i * (x_i @ x_i) = 1001803.87500\n",
      "Position 29: y_i * (x_i @ x_i) = 1003177.62500\n",
      "Position 30: y_i * (x_i @ x_i) = -1001290.06250\n",
      "Position 31: y_i * (x_i @ x_i) = -997153.06250\n",
      "Position 32: y_i * (x_i @ x_i) = 1000729.12500\n",
      "Position 33: y_i * (x_i @ x_i) = -1002457.81250\n",
      "Position 34: y_i * (x_i @ x_i) = -997601.93750\n",
      "Position 35: y_i * (x_i @ x_i) = -1001789.43750\n",
      "Position 36: y_i * (x_i @ x_i) = 1002077.56250\n",
      "Position 37: y_i * (x_i @ x_i) = 997738.25000\n",
      "Position 38: y_i * (x_i @ x_i) = -1001043.93750\n",
      "Position 39: y_i * (x_i @ x_i) = 1004141.06250\n",
      "Position 40: y_i * (x_i @ x_i) = 1001513.31250\n",
      "Position 41: y_i * (x_i @ x_i) = -1002097.75000\n",
      "Position 42: y_i * (x_i @ x_i) = 998634.87500\n",
      "Position 43: y_i * (x_i @ x_i) = 1001361.68750\n",
      "Position 44: y_i * (x_i @ x_i) = 998624.62500\n",
      "Position 45: y_i * (x_i @ x_i) = 1000785.81250\n",
      "Position 46: y_i * (x_i @ x_i) = -1003430.00000\n",
      "Position 47: y_i * (x_i @ x_i) = -1001600.87500\n",
      "Position 48: y_i * (x_i @ x_i) = -1001668.37500\n",
      "Position 49: y_i * (x_i @ x_i) = -1001497.25000\n"
     ]
    }
   ],
   "source": [
    "def test_numerical_precision():\n",
    "    \"\"\"Test to examine numerical precision effects with different R values\"\"\"\n",
    "    # Setup parameters\n",
    "    d = 1000  \n",
    "    N = 50  # Longer sequence to see effects\n",
    "    B = 100   \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Test both R values\n",
    "    R_values = [1.0, d]\n",
    "    \n",
    "    for R in R_values:\n",
    "        print(f\"\\n=== Testing R = {R} ===\")\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = GaussianMixtureDataset(d, N, B, R, label_flip_p=0.15)\n",
    "        context_x, context_y, _, _ = [t.to(device) for t in dataset[0]]\n",
    "        \n",
    "        # Create model and set W to identity\n",
    "        model = LinearTransformer(d).to(device)\n",
    "        with torch.no_grad():\n",
    "            model.W.data = torch.eye(d, device=device)\n",
    "        \n",
    "        # Examine quantities for different sequence lengths\n",
    "        for k in [1, 5, 10, 20, 50]:\n",
    "            print(f\"\\nSequence length k = {k}\")\n",
    "            curr_x = context_x[:, :k]\n",
    "            curr_y = context_y[:, :k]\n",
    "            \n",
    "            # Compute components explicitly\n",
    "            curr_y_signal = 2 * curr_y - 1\n",
    "            \n",
    "            # Examine x magnitudes\n",
    "            x_norms = torch.norm(curr_x, dim=2)  # (B, k)\n",
    "            print(f\"X norms - mean: {x_norms.mean():.2f}, std: {x_norms.std():.2f}\")\n",
    "            \n",
    "            # Examine contribution to mean\n",
    "            contributions = curr_y_signal[..., None] * curr_x  # (B, k, d)\n",
    "            contrib_norms = torch.norm(contributions, dim=2)  # (B, k)\n",
    "            print(f\"Contribution norms - mean: {contrib_norms.mean():.2f}, std: {contrib_norms.std():.2f}\")\n",
    "            \n",
    "            # Examine hat_mu\n",
    "            hat_mu = (1/k) * torch.sum(contributions, dim=1)  # (B, d)\n",
    "            mu_norms = torch.norm(hat_mu, dim=1)  # (B,)\n",
    "            print(f\"hat_mu norms - mean: {mu_norms.mean():.2f}, std: {mu_norms.std():.2f}\")\n",
    "            \n",
    "            # Examine logits before thresholding\n",
    "            logits = (hat_mu[:, None, :] @ curr_x.transpose(-1, -2))[:, 0, :]  # (B, k)\n",
    "            print(f\"Logit magnitudes - mean: {logits.abs().mean():.2f}, std: {logits.std():.2f}\")\n",
    "            print(f\"Logit range: [{logits.min():.2f}, {logits.max():.2f}]\")\n",
    "            \n",
    "            # Get predictions and check accuracy\n",
    "            preds = model.compute_in_context_preds(curr_x, curr_y)\n",
    "            accuracy = (preds == curr_y).float().mean().item()\n",
    "            print(f\"Accuracy: {accuracy:.3f}\")\n",
    "            \n",
    "            # Check if reconstructions are exact\n",
    "            # Get a few examples where prediction failed\n",
    "            if accuracy < 1.0:\n",
    "                failed_cases = (preds != curr_y).any(dim=1)\n",
    "                b_idx = failed_cases.nonzero()[0].item()\n",
    "                print(\"\\nExample of failed reconstruction:\")\n",
    "                print(f\"Original y: {curr_y[b_idx].cpu().numpy()}\")\n",
    "                print(f\"Predicted:  {preds[b_idx].cpu().numpy()}\")\n",
    "                print(f\"Logits:     {logits[b_idx].cpu().numpy()}\")\n",
    "                \n",
    "                # Examine exact y_i * x_i @ x_i values for this case\n",
    "                for i in range(k):\n",
    "                    x_i = curr_x[b_idx, i]\n",
    "                    y_i = curr_y_signal[b_idx, i]\n",
    "                    recon = y_i * (x_i @ x_i)\n",
    "                    print(f\"Position {i}: y_i * (x_i @ x_i) = {recon:.5f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_numerical_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating with identity model: d=50, N=200\n",
      "Using R = 6.47\n",
      "k=1: accuracy=0.460\n",
      "  Signal norm at position 1: 8.911\n",
      "  Example 0: pred=1.0, true=0.0\n",
      "  Example 1: pred=0.0, true=0.0\n",
      "  Example 2: pred=0.0, true=1.0\n",
      "k=2: accuracy=0.440\n",
      "  Signal norm at position 2: 7.665\n",
      "  Example 0: pred=0.0, true=1.0\n",
      "  Example 1: pred=0.0, true=0.0\n",
      "  Example 2: pred=1.0, true=1.0\n",
      "k=3: accuracy=0.510\n",
      "  Signal norm at position 3: 7.490\n",
      "  Example 0: pred=1.0, true=1.0\n",
      "  Example 1: pred=0.0, true=1.0\n",
      "  Example 2: pred=1.0, true=0.0\n",
      "k=4: accuracy=0.580\n",
      "k=5: accuracy=0.590\n",
      "k=6: accuracy=0.510\n",
      "k=7: accuracy=0.460\n",
      "k=8: accuracy=0.610\n",
      "k=9: accuracy=0.520\n",
      "k=10: accuracy=0.400\n",
      "k=11: accuracy=0.490\n",
      "k=12: accuracy=0.490\n",
      "k=13: accuracy=0.510\n",
      "k=14: accuracy=0.510\n",
      "k=15: accuracy=0.540\n",
      "k=16: accuracy=0.510\n",
      "k=17: accuracy=0.560\n",
      "k=18: accuracy=0.470\n",
      "k=19: accuracy=0.510\n",
      "k=20: accuracy=0.540\n",
      "k=21: accuracy=0.500\n",
      "k=22: accuracy=0.520\n",
      "k=23: accuracy=0.370\n",
      "k=24: accuracy=0.410\n",
      "k=25: accuracy=0.610\n",
      "k=26: accuracy=0.470\n",
      "k=27: accuracy=0.580\n",
      "k=28: accuracy=0.530\n",
      "k=29: accuracy=0.470\n",
      "k=30: accuracy=0.490\n",
      "k=31: accuracy=0.500\n",
      "k=32: accuracy=0.470\n",
      "k=33: accuracy=0.490\n",
      "k=34: accuracy=0.490\n",
      "k=35: accuracy=0.520\n",
      "k=36: accuracy=0.530\n",
      "k=37: accuracy=0.510\n",
      "k=38: accuracy=0.540\n",
      "k=39: accuracy=0.520\n",
      "k=40: accuracy=0.570\n",
      "k=41: accuracy=0.510\n",
      "k=42: accuracy=0.550\n",
      "k=43: accuracy=0.530\n",
      "k=44: accuracy=0.560\n",
      "k=45: accuracy=0.580\n",
      "k=46: accuracy=0.500\n",
      "k=47: accuracy=0.490\n",
      "k=48: accuracy=0.590\n",
      "k=49: accuracy=0.410\n",
      "k=50: accuracy=0.490\n",
      "k=51: accuracy=0.510\n",
      "k=52: accuracy=0.430\n",
      "k=53: accuracy=0.500\n",
      "k=54: accuracy=0.520\n",
      "k=55: accuracy=0.460\n",
      "k=56: accuracy=0.500\n",
      "k=57: accuracy=0.480\n",
      "k=58: accuracy=0.550\n",
      "k=59: accuracy=0.430\n",
      "k=60: accuracy=0.520\n",
      "k=61: accuracy=0.530\n",
      "k=62: accuracy=0.440\n",
      "k=63: accuracy=0.490\n",
      "k=64: accuracy=0.490\n",
      "k=65: accuracy=0.440\n",
      "k=66: accuracy=0.580\n",
      "k=67: accuracy=0.520\n",
      "k=68: accuracy=0.510\n",
      "k=69: accuracy=0.510\n",
      "k=70: accuracy=0.500\n",
      "k=71: accuracy=0.480\n",
      "k=72: accuracy=0.440\n",
      "k=73: accuracy=0.400\n",
      "k=74: accuracy=0.470\n",
      "k=75: accuracy=0.490\n",
      "k=76: accuracy=0.520\n",
      "k=77: accuracy=0.490\n",
      "k=78: accuracy=0.530\n",
      "k=79: accuracy=0.500\n",
      "k=80: accuracy=0.470\n",
      "k=81: accuracy=0.440\n",
      "k=82: accuracy=0.510\n",
      "k=83: accuracy=0.550\n",
      "k=84: accuracy=0.600\n",
      "k=85: accuracy=0.520\n",
      "k=86: accuracy=0.440\n",
      "k=87: accuracy=0.520\n",
      "k=88: accuracy=0.520\n",
      "k=89: accuracy=0.420\n",
      "k=90: accuracy=0.520\n",
      "k=91: accuracy=0.580\n",
      "k=92: accuracy=0.530\n",
      "k=93: accuracy=0.490\n",
      "k=94: accuracy=0.540\n",
      "k=95: accuracy=0.460\n",
      "k=96: accuracy=0.430\n",
      "k=97: accuracy=0.580\n",
      "k=98: accuracy=0.510\n",
      "k=99: accuracy=0.540\n",
      "k=100: accuracy=0.520\n",
      "k=101: accuracy=0.480\n",
      "k=102: accuracy=0.550\n",
      "k=103: accuracy=0.470\n",
      "k=104: accuracy=0.470\n",
      "k=105: accuracy=0.490\n",
      "k=106: accuracy=0.490\n",
      "k=107: accuracy=0.480\n",
      "k=108: accuracy=0.440\n",
      "k=109: accuracy=0.510\n",
      "k=110: accuracy=0.480\n",
      "k=111: accuracy=0.460\n",
      "k=112: accuracy=0.490\n",
      "k=113: accuracy=0.520\n",
      "k=114: accuracy=0.500\n",
      "k=115: accuracy=0.480\n",
      "k=116: accuracy=0.540\n",
      "k=117: accuracy=0.560\n",
      "k=118: accuracy=0.510\n",
      "k=119: accuracy=0.440\n",
      "k=120: accuracy=0.420\n",
      "k=121: accuracy=0.530\n",
      "k=122: accuracy=0.490\n",
      "k=123: accuracy=0.530\n",
      "k=124: accuracy=0.480\n",
      "k=125: accuracy=0.540\n",
      "k=126: accuracy=0.450\n",
      "k=127: accuracy=0.520\n",
      "k=128: accuracy=0.500\n",
      "k=129: accuracy=0.580\n",
      "k=130: accuracy=0.530\n",
      "k=131: accuracy=0.510\n",
      "k=132: accuracy=0.430\n",
      "k=133: accuracy=0.530\n",
      "k=134: accuracy=0.480\n",
      "k=135: accuracy=0.530\n",
      "k=136: accuracy=0.410\n",
      "k=137: accuracy=0.570\n",
      "k=138: accuracy=0.600\n",
      "k=139: accuracy=0.540\n",
      "k=140: accuracy=0.480\n",
      "k=141: accuracy=0.490\n",
      "k=142: accuracy=0.480\n",
      "k=143: accuracy=0.530\n",
      "k=144: accuracy=0.510\n",
      "k=145: accuracy=0.460\n",
      "k=146: accuracy=0.490\n",
      "k=147: accuracy=0.540\n",
      "k=148: accuracy=0.510\n",
      "k=149: accuracy=0.520\n",
      "k=150: accuracy=0.440\n",
      "k=151: accuracy=0.560\n",
      "k=152: accuracy=0.510\n",
      "k=153: accuracy=0.540\n",
      "k=154: accuracy=0.530\n",
      "k=155: accuracy=0.580\n",
      "k=156: accuracy=0.610\n",
      "k=157: accuracy=0.560\n",
      "k=158: accuracy=0.500\n",
      "k=159: accuracy=0.520\n",
      "k=160: accuracy=0.540\n",
      "k=161: accuracy=0.520\n",
      "k=162: accuracy=0.530\n",
      "k=163: accuracy=0.460\n",
      "k=164: accuracy=0.520\n",
      "k=165: accuracy=0.480\n",
      "k=166: accuracy=0.590\n",
      "k=167: accuracy=0.480\n",
      "k=168: accuracy=0.500\n",
      "k=169: accuracy=0.460\n",
      "k=170: accuracy=0.460\n",
      "k=171: accuracy=0.490\n",
      "k=172: accuracy=0.480\n",
      "k=173: accuracy=0.490\n",
      "k=174: accuracy=0.500\n",
      "k=175: accuracy=0.450\n",
      "k=176: accuracy=0.560\n",
      "k=177: accuracy=0.550\n",
      "k=178: accuracy=0.570\n",
      "k=179: accuracy=0.460\n",
      "k=180: accuracy=0.410\n",
      "k=181: accuracy=0.490\n",
      "k=182: accuracy=0.480\n",
      "k=183: accuracy=0.550\n",
      "k=184: accuracy=0.520\n",
      "k=185: accuracy=0.400\n",
      "k=186: accuracy=0.540\n",
      "k=187: accuracy=0.460\n",
      "k=188: accuracy=0.520\n",
      "k=189: accuracy=0.550\n",
      "k=190: accuracy=0.500\n",
      "k=191: accuracy=0.540\n",
      "k=192: accuracy=0.420\n",
      "k=193: accuracy=0.500\n",
      "k=194: accuracy=0.580\n",
      "k=195: accuracy=0.430\n",
      "k=196: accuracy=0.530\n",
      "k=197: accuracy=0.480\n",
      "k=198: accuracy=0.450\n",
      "k=199: accuracy=0.510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46000000834465027,\n",
       " 0.4399999976158142,\n",
       " 0.5099999904632568,\n",
       " 0.5799999833106995,\n",
       " 0.5899999737739563,\n",
       " 0.5099999904632568,\n",
       " 0.46000000834465027,\n",
       " 0.6100000143051147,\n",
       " 0.5199999809265137,\n",
       " 0.4000000059604645,\n",
       " 0.49000000953674316,\n",
       " 0.49000000953674316,\n",
       " 0.5099999904632568,\n",
       " 0.5099999904632568,\n",
       " 0.5400000214576721,\n",
       " 0.5099999904632568,\n",
       " 0.5600000023841858,\n",
       " 0.4699999988079071,\n",
       " 0.5099999904632568,\n",
       " 0.5400000214576721,\n",
       " 0.5,\n",
       " 0.5199999809265137,\n",
       " 0.3700000047683716,\n",
       " 0.4099999964237213,\n",
       " 0.6100000143051147,\n",
       " 0.4699999988079071,\n",
       " 0.5799999833106995,\n",
       " 0.5299999713897705,\n",
       " 0.4699999988079071,\n",
       " 0.49000000953674316,\n",
       " 0.5,\n",
       " 0.4699999988079071,\n",
       " 0.49000000953674316,\n",
       " 0.49000000953674316,\n",
       " 0.5199999809265137,\n",
       " 0.5299999713897705,\n",
       " 0.5099999904632568,\n",
       " 0.5400000214576721,\n",
       " 0.5199999809265137,\n",
       " 0.5699999928474426,\n",
       " 0.5099999904632568,\n",
       " 0.550000011920929,\n",
       " 0.5299999713897705,\n",
       " 0.5600000023841858,\n",
       " 0.5799999833106995,\n",
       " 0.5,\n",
       " 0.49000000953674316,\n",
       " 0.5899999737739563,\n",
       " 0.4099999964237213,\n",
       " 0.49000000953674316,\n",
       " 0.5099999904632568,\n",
       " 0.4300000071525574,\n",
       " 0.5,\n",
       " 0.5199999809265137,\n",
       " 0.46000000834465027,\n",
       " 0.5,\n",
       " 0.47999998927116394,\n",
       " 0.550000011920929,\n",
       " 0.4300000071525574,\n",
       " 0.5199999809265137,\n",
       " 0.5299999713897705,\n",
       " 0.4399999976158142,\n",
       " 0.49000000953674316,\n",
       " 0.49000000953674316,\n",
       " 0.4399999976158142,\n",
       " 0.5799999833106995,\n",
       " 0.5199999809265137,\n",
       " 0.5099999904632568,\n",
       " 0.5099999904632568,\n",
       " 0.5,\n",
       " 0.47999998927116394,\n",
       " 0.4399999976158142,\n",
       " 0.4000000059604645,\n",
       " 0.4699999988079071,\n",
       " 0.49000000953674316,\n",
       " 0.5199999809265137,\n",
       " 0.49000000953674316,\n",
       " 0.5299999713897705,\n",
       " 0.5,\n",
       " 0.4699999988079071,\n",
       " 0.4399999976158142,\n",
       " 0.5099999904632568,\n",
       " 0.550000011920929,\n",
       " 0.6000000238418579,\n",
       " 0.5199999809265137,\n",
       " 0.4399999976158142,\n",
       " 0.5199999809265137,\n",
       " 0.5199999809265137,\n",
       " 0.41999998688697815,\n",
       " 0.5199999809265137,\n",
       " 0.5799999833106995,\n",
       " 0.5299999713897705,\n",
       " 0.49000000953674316,\n",
       " 0.5400000214576721,\n",
       " 0.46000000834465027,\n",
       " 0.4300000071525574,\n",
       " 0.5799999833106995,\n",
       " 0.5099999904632568,\n",
       " 0.5400000214576721,\n",
       " 0.5199999809265137,\n",
       " 0.47999998927116394,\n",
       " 0.550000011920929,\n",
       " 0.4699999988079071,\n",
       " 0.4699999988079071,\n",
       " 0.49000000953674316,\n",
       " 0.49000000953674316,\n",
       " 0.47999998927116394,\n",
       " 0.4399999976158142,\n",
       " 0.5099999904632568,\n",
       " 0.47999998927116394,\n",
       " 0.46000000834465027,\n",
       " 0.49000000953674316,\n",
       " 0.5199999809265137,\n",
       " 0.5,\n",
       " 0.47999998927116394,\n",
       " 0.5400000214576721,\n",
       " 0.5600000023841858,\n",
       " 0.5099999904632568,\n",
       " 0.4399999976158142,\n",
       " 0.41999998688697815,\n",
       " 0.5299999713897705,\n",
       " 0.49000000953674316,\n",
       " 0.5299999713897705,\n",
       " 0.47999998927116394,\n",
       " 0.5400000214576721,\n",
       " 0.44999998807907104,\n",
       " 0.5199999809265137,\n",
       " 0.5,\n",
       " 0.5799999833106995,\n",
       " 0.5299999713897705,\n",
       " 0.5099999904632568,\n",
       " 0.4300000071525574,\n",
       " 0.5299999713897705,\n",
       " 0.47999998927116394,\n",
       " 0.5299999713897705,\n",
       " 0.4099999964237213,\n",
       " 0.5699999928474426,\n",
       " 0.6000000238418579,\n",
       " 0.5400000214576721,\n",
       " 0.47999998927116394,\n",
       " 0.49000000953674316,\n",
       " 0.47999998927116394,\n",
       " 0.5299999713897705,\n",
       " 0.5099999904632568,\n",
       " 0.46000000834465027,\n",
       " 0.49000000953674316,\n",
       " 0.5400000214576721,\n",
       " 0.5099999904632568,\n",
       " 0.5199999809265137,\n",
       " 0.4399999976158142,\n",
       " 0.5600000023841858,\n",
       " 0.5099999904632568,\n",
       " 0.5400000214576721,\n",
       " 0.5299999713897705,\n",
       " 0.5799999833106995,\n",
       " 0.6100000143051147,\n",
       " 0.5600000023841858,\n",
       " 0.5,\n",
       " 0.5199999809265137,\n",
       " 0.5400000214576721,\n",
       " 0.5199999809265137,\n",
       " 0.5299999713897705,\n",
       " 0.46000000834465027,\n",
       " 0.5199999809265137,\n",
       " 0.47999998927116394,\n",
       " 0.5899999737739563,\n",
       " 0.47999998927116394,\n",
       " 0.5,\n",
       " 0.46000000834465027,\n",
       " 0.46000000834465027,\n",
       " 0.49000000953674316,\n",
       " 0.47999998927116394,\n",
       " 0.49000000953674316,\n",
       " 0.5,\n",
       " 0.44999998807907104,\n",
       " 0.5600000023841858,\n",
       " 0.550000011920929,\n",
       " 0.5699999928474426,\n",
       " 0.46000000834465027,\n",
       " 0.4099999964237213,\n",
       " 0.49000000953674316,\n",
       " 0.47999998927116394,\n",
       " 0.550000011920929,\n",
       " 0.5199999809265137,\n",
       " 0.4000000059604645,\n",
       " 0.5400000214576721,\n",
       " 0.46000000834465027,\n",
       " 0.5199999809265137,\n",
       " 0.550000011920929,\n",
       " 0.5,\n",
       " 0.5400000214576721,\n",
       " 0.41999998688697815,\n",
       " 0.5,\n",
       " 0.5799999833106995,\n",
       " 0.4300000071525574,\n",
       " 0.5299999713897705,\n",
       " 0.47999998927116394,\n",
       " 0.44999998807907104,\n",
       " 0.5099999904632568]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def validate_with_identity_model(d: int, N: int = 50, num_samples: int = 1000):\n",
    "    \"\"\"\n",
    "    Validate evaluation code using a model with identity weight matrix.\n",
    "    This should perform well when signal-to-noise ratio is high enough.\n",
    "    \"\"\"\n",
    "    print(f\"\\nValidating with identity model: d={d}, N={N}\")\n",
    "    \n",
    "    # Create model and set weights to identity\n",
    "    model = LinearTransformer(d)\n",
    "    with torch.no_grad():\n",
    "        model.W.copy_(torch.eye(d))\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dataset with high signal-to-noise ratio\n",
    "    R = 2*d**0.3 # High SNR regime\n",
    "    print(f\"Using R = {R:.2f}\")\n",
    "    \n",
    "    dataset = GaussianMixtureDataset(\n",
    "        d=d,\n",
    "        N=N,\n",
    "        B=num_samples,\n",
    "        R=R,\n",
    "        is_validation=True,\n",
    "        label_flip_p=0.0\n",
    "    )\n",
    "    \n",
    "    # Get data\n",
    "    context_x, context_y, _, _ = dataset[0]\n",
    "    accuracies = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Test prediction at each sequence length\n",
    "        for k in range(1, N):\n",
    "            curr_context_x = context_x[:, :k]\n",
    "            curr_context_y = context_y[:, :k]\n",
    "            next_y = context_y[:, k]\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = model.compute_in_context_preds(curr_context_x, curr_context_y)\n",
    "            accuracy = (preds[:, -1] == next_y).float().mean().item()\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "            print(f\"k={k}: accuracy={accuracy:.3f}\")\n",
    "            \n",
    "            # Additional debugging for first few iterations\n",
    "            if k <= 3:\n",
    "                # Check intermediate values\n",
    "                batch_idx = 0  # Look at first sequence in batch\n",
    "                context_term = (1/k) * torch.sum(\n",
    "                    (2 * curr_context_y[batch_idx] - 1).unsqueeze(-1) * curr_context_x[batch_idx], \n",
    "                    dim=0\n",
    "                )\n",
    "                signal_norm = torch.norm(context_term).item()\n",
    "                print(f\"  Signal norm at position {k}: {signal_norm:.3f}\")\n",
    "                \n",
    "                # Print a few example predictions\n",
    "                for i in range(min(3, len(next_y))):\n",
    "                    pred = preds[i, -1].item()\n",
    "                    true_y = next_y[i].item()\n",
    "                    print(f\"  Example {i}: pred={pred:.1f}, true={true_y}\")\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "validate_with_identity_model(d=50, N=200, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debugging identity model with d=5000, N=20, B=1000\n",
      "R = 25.75\n",
      "\n",
      "k = 1\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=75.252, std=0.753\n",
      "Noise level stats: mean=75.257, std=0.752\n",
      "Dot product stats: mean=-22.941, std=670.401\n",
      "\n",
      "Example 0:\n",
      "True next y: 0.0\n",
      "Predicted y: 0.0\n",
      "Logit: -655.774\n",
      "Signal norm: 75.684\n",
      "Dot product: -655.774\n",
      "Alignment with true direction: 0.117\n",
      "\n",
      "Example 1:\n",
      "True next y: 0.0\n",
      "Predicted y: 0.0\n",
      "Logit: -610.141\n",
      "Signal norm: 75.443\n",
      "Dot product: -610.141\n",
      "\n",
      "Example 2:\n",
      "True next y: 1.0\n",
      "Predicted y: 1.0\n",
      "Logit: 695.984\n",
      "Signal norm: 74.590\n",
      "Dot product: 695.984\n",
      "\n",
      "k = 2\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=56.254, std=0.560\n",
      "Noise level stats: mean=75.232, std=0.760\n",
      "Dot product stats: mean=-4.550, std=666.523\n",
      "\n",
      "Example 0:\n",
      "True next y: 0.0\n",
      "Predicted y: 0.0\n",
      "Logit: -616.523\n",
      "Signal norm: 55.962\n",
      "Dot product: -616.523\n",
      "Alignment with true direction: 0.148\n",
      "\n",
      "Example 1:\n",
      "True next y: 1.0\n",
      "Predicted y: 1.0\n",
      "Logit: 682.434\n",
      "Signal norm: 55.825\n",
      "Dot product: 682.434\n",
      "\n",
      "Example 2:\n",
      "True next y: 0.0\n",
      "Predicted y: 0.0\n",
      "Logit: -689.105\n",
      "Signal norm: 56.103\n",
      "Dot product: -689.105\n",
      "\n",
      "k = 3\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=48.273, std=0.461\n",
      "Noise level stats: mean=75.274, std=0.721\n",
      "Dot product stats: mean=-15.433, std=665.002\n",
      "\n",
      "Example 0:\n",
      "True next y: 0.0\n",
      "Predicted y: 0.0\n",
      "Logit: -605.013\n",
      "Signal norm: 47.792\n",
      "Dot product: -605.013\n",
      "Alignment with true direction: 0.167\n",
      "\n",
      "Example 1:\n",
      "True next y: 1.0\n",
      "Predicted y: 1.0\n",
      "Logit: 652.558\n",
      "Signal norm: 48.085\n",
      "Dot product: 652.558\n",
      "\n",
      "Example 2:\n",
      "True next y: 0.0\n",
      "Predicted y: 0.0\n",
      "Logit: -631.456\n",
      "Signal norm: 48.541\n",
      "Dot product: -631.456\n",
      "\n",
      "k = 4\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=43.744, std=0.413\n",
      "Noise level stats: mean=75.261, std=0.776\n",
      "Dot product stats: mean=-14.030, std=667.833\n",
      "\n",
      "k = 5\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=40.797, std=0.372\n",
      "Noise level stats: mean=75.269, std=0.764\n",
      "Dot product stats: mean=5.980, std=665.350\n",
      "\n",
      "k = 6\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=38.698, std=0.352\n",
      "Noise level stats: mean=75.235, std=0.766\n",
      "Dot product stats: mean=-21.811, std=664.801\n",
      "\n",
      "k = 7\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=37.125, std=0.323\n",
      "Noise level stats: mean=75.274, std=0.728\n",
      "Dot product stats: mean=5.083, std=664.994\n",
      "\n",
      "k = 8\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=35.901, std=0.305\n",
      "Noise level stats: mean=75.292, std=0.753\n",
      "Dot product stats: mean=-5.270, std=664.784\n",
      "\n",
      "k = 9\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=34.920, std=0.288\n",
      "Noise level stats: mean=75.259, std=0.759\n",
      "Dot product stats: mean=-7.954, std=665.781\n",
      "\n",
      "k = 10\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=34.117, std=0.278\n",
      "Noise level stats: mean=75.262, std=0.739\n",
      "Dot product stats: mean=2.133, std=665.943\n",
      "\n",
      "k = 11\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=33.446, std=0.262\n",
      "Noise level stats: mean=75.260, std=0.736\n",
      "Dot product stats: mean=-8.916, std=665.494\n",
      "\n",
      "k = 12\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=32.876, std=0.251\n",
      "Noise level stats: mean=75.283, std=0.755\n",
      "Dot product stats: mean=20.792, std=664.035\n",
      "\n",
      "k = 13\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=32.383, std=0.244\n",
      "Noise level stats: mean=75.228, std=0.732\n",
      "Dot product stats: mean=15.442, std=663.206\n",
      "\n",
      "k = 14\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=31.952, std=0.233\n",
      "Noise level stats: mean=75.227, std=0.766\n",
      "Dot product stats: mean=13.993, std=662.861\n",
      "\n",
      "k = 15\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=31.573, std=0.227\n",
      "Noise level stats: mean=75.243, std=0.768\n",
      "Dot product stats: mean=6.199, std=664.779\n",
      "\n",
      "k = 16\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=31.242, std=0.217\n",
      "Noise level stats: mean=75.272, std=0.752\n",
      "Dot product stats: mean=11.713, std=664.447\n",
      "\n",
      "k = 17\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=30.946, std=0.211\n",
      "Noise level stats: mean=75.257, std=0.788\n",
      "Dot product stats: mean=-24.650, std=664.498\n",
      "\n",
      "k = 18\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=30.681, std=0.204\n",
      "Noise level stats: mean=75.231, std=0.768\n",
      "Dot product stats: mean=-20.256, std=664.668\n",
      "\n",
      "k = 19\n",
      "Accuracy: 1.000\n",
      "Signal norm stats: mean=30.443, std=0.199\n",
      "Noise level stats: mean=75.254, std=0.772\n",
      "Dot product stats: mean=-18.260, std=662.571\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from classification_icl import LinearTransformer\n",
    "from typing import Tuple\n",
    "\n",
    "def generate_test_data(d: int, N: int, B: int, R: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate test data with known signal structure to verify model behavior.\n",
    "    \"\"\"\n",
    "    # Generate mean vectors\n",
    "    mus = torch.randn(B, d)\n",
    "    mus = mus / torch.norm(mus, dim=1, keepdim=True) * R\n",
    "    \n",
    "    # Generate labels\n",
    "    y = (torch.rand(B, N) > 0.5).float()\n",
    "    y_signal = 2 * y - 1  # Convert to 1\n",
    "    \n",
    "    # Generate inputs with signal\n",
    "    z = torch.randn(B, N, d)  # noise\n",
    "    x = y_signal.unsqueeze(-1) * mus.unsqueeze(1) + z\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def debug_identity_model(d: int = 5000, N: int = 20, B: int = 1000):\n",
    "    \"\"\"\n",
    "    Debug the identity model with detailed computation checks.\n",
    "    \"\"\"\n",
    "    print(f\"\\nDebugging identity model with d={d}, N={N}, B={B}\")\n",
    "    \n",
    "    # Set up model with identity matrix\n",
    "    model = LinearTransformer(d)\n",
    "    with torch.no_grad():\n",
    "        model.W.copy_(torch.eye(d))\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate data with known structure\n",
    "    R = 2 * d**0.3\n",
    "    print(f\"R = {R:.2f}\")\n",
    "    x, y = generate_test_data(d, N, B, R)\n",
    "    \n",
    "    # Debug computations step by step\n",
    "    with torch.no_grad():\n",
    "        for k in range(1, N):\n",
    "            print(f\"\\nk = {k}\")\n",
    "            \n",
    "            # Get current context and next example\n",
    "            context_x = x[:, :k]  # (B, k, d)\n",
    "            context_y = y[:, :k]  # (B, k)\n",
    "            next_x = x[:, k]      # (B, d)\n",
    "            next_y = y[:, k]      # (B)\n",
    "            \n",
    "            # Manual computation for clarity\n",
    "            # 1. Convert labels to 1\n",
    "            y_signal = 2 * context_y - 1  # (B, k)\n",
    "            \n",
    "            # 2. Compute context term (1/k)y_i x_i\n",
    "            context_term = (1/k) * torch.sum(y_signal.unsqueeze(-1) * context_x, dim=1)  # (B, d)\n",
    "            \n",
    "            # 3. Since W is identity, transformed = context_term\n",
    "            # 4. Compute prediction logit\n",
    "            logits = torch.sum(context_term * next_x, dim=1)  # (B)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            preds = (logits > 0).float()\n",
    "            acc = (preds == next_y).float().mean().item()\n",
    "            \n",
    "            # Detailed diagnostics\n",
    "            signal_norm = torch.norm(context_term, dim=1)\n",
    "            noise_level = torch.norm(next_x, dim=1)\n",
    "            dot_products = torch.sum(context_term * next_x, dim=1)\n",
    "            \n",
    "            print(f\"Accuracy: {acc:.3f}\")\n",
    "            print(f\"Signal norm stats: mean={signal_norm.mean():.3f}, std={signal_norm.std():.3f}\")\n",
    "            print(f\"Noise level stats: mean={noise_level.mean():.3f}, std={noise_level.std():.3f}\")\n",
    "            print(f\"Dot product stats: mean={dot_products.mean():.3f}, std={dot_products.std():.3f}\")\n",
    "            \n",
    "            # Look at a few examples in detail\n",
    "            if k <= 3:\n",
    "                for i in range(3):\n",
    "                    print(f\"\\nExample {i}:\")\n",
    "                    print(f\"True next y: {next_y[i].item()}\")\n",
    "                    print(f\"Predicted y: {preds[i].item()}\")\n",
    "                    print(f\"Logit: {logits[i].item():.3f}\")\n",
    "                    print(f\"Signal norm: {signal_norm[i].item():.3f}\")\n",
    "                    print(f\"Dot product: {dot_products[i].item():.3f}\")\n",
    "                    # Check alignment between context_term and true mu\n",
    "                    if i == 0:\n",
    "                        # Estimate mu from next_x using true label\n",
    "                        true_signal = next_y[i].item() * 2 - 1\n",
    "                        estimated_mu = true_signal * next_x[i] / R\n",
    "                        alignment = torch.dot(context_term[i], estimated_mu) / (torch.norm(context_term[i]) * torch.norm(estimated_mu))\n",
    "                        print(f\"Alignment with true direction: {alignment.item():.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    debug_identity_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing identity matrix: d=10, N=200, B=100, R=1.26\n",
      "\n",
      "Verifying signal and noise properties:\n",
      "Signal norms: mean=1.259, std=0.000\n",
      "Noise norms: mean=3.087, std=0.692\n",
      "\n",
      "k = 1\n",
      "Accuracy: 0.710\n",
      "Signal norm: mean=3.255, std=0.666\n",
      "\n",
      "Example 0:\n",
      "True y: 1.0\n",
      "Pred y: 1.0\n",
      "Logit: 0.820\n",
      "Signal norm: 3.008\n",
      "Alignment with true mu: 0.015\n",
      "\n",
      "Example 1:\n",
      "True y: 0.0\n",
      "Pred y: 0.0\n",
      "Logit: -1.353\n",
      "Signal norm: 3.446\n",
      "Alignment with true mu: 0.263\n",
      "\n",
      "Example 2:\n",
      "True y: 1.0\n",
      "Pred y: 1.0\n",
      "Logit: 6.998\n",
      "Signal norm: 3.700\n",
      "Alignment with true mu: 0.149\n",
      "\n",
      "k = 2\n",
      "Accuracy: 0.670\n",
      "Signal norm: mean=2.519, std=0.609\n",
      "\n",
      "Example 0:\n",
      "True y: 1.0\n",
      "Pred y: 0.0\n",
      "Logit: -0.719\n",
      "Signal norm: 2.393\n",
      "Alignment with true mu: 0.521\n",
      "\n",
      "Example 1:\n",
      "True y: 0.0\n",
      "Pred y: 1.0\n",
      "Logit: 1.792\n",
      "Signal norm: 2.315\n",
      "Alignment with true mu: 0.251\n",
      "\n",
      "Example 2:\n",
      "True y: 0.0\n",
      "Pred y: 0.0\n",
      "Logit: -1.944\n",
      "Signal norm: 3.212\n",
      "Alignment with true mu: 0.418\n",
      "\n",
      "k = 3\n",
      "Accuracy: 0.800\n",
      "Signal norm: mean=2.186, std=0.488\n",
      "\n",
      "Example 0:\n",
      "True y: 1.0\n",
      "Pred y: 1.0\n",
      "Logit: 1.822\n",
      "Signal norm: 1.913\n",
      "Alignment with true mu: 0.675\n",
      "\n",
      "Example 1:\n",
      "True y: 0.0\n",
      "Pred y: 1.0\n",
      "Logit: 0.279\n",
      "Signal norm: 1.604\n",
      "Alignment with true mu: 0.502\n",
      "\n",
      "Example 2:\n",
      "True y: 0.0\n",
      "Pred y: 1.0\n",
      "Logit: 3.047\n",
      "Signal norm: 2.646\n",
      "Alignment with true mu: 0.574\n",
      "\n",
      "k = 4\n",
      "Accuracy: 0.770\n",
      "Signal norm: mean=1.997, std=0.416\n",
      "\n",
      "Example 0:\n",
      "True y: 0.0\n",
      "Pred y: 0.0\n",
      "Logit: -2.878\n",
      "Signal norm: 1.867\n",
      "Alignment with true mu: 0.748\n",
      "\n",
      "Example 1:\n",
      "True y: 1.0\n",
      "Pred y: 1.0\n",
      "Logit: 2.203\n",
      "Signal norm: 1.339\n",
      "Alignment with true mu: 0.586\n",
      "\n",
      "Example 2:\n",
      "True y: 0.0\n",
      "Pred y: 0.0\n",
      "Logit: -2.415\n",
      "Signal norm: 1.761\n",
      "Alignment with true mu: 0.551\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from classification_icl import LinearTransformer\n",
    "\n",
    "def test_identity_simple(d=10, N=200, B=100):\n",
    "    \"\"\"\n",
    "    Simple test of identity matrix behavior with clear signal structure\n",
    "    \"\"\"\n",
    "    # Create model with identity matrix\n",
    "    model = LinearTransformer(d)\n",
    "    with torch.no_grad():\n",
    "        model.W.copy_(torch.eye(d))\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate data with known signal\n",
    "    # R = 2 * d**0.3\n",
    "    R = d**0.1\n",
    "    print(f\"\\nTesting identity matrix: d={d}, N={N}, B={B}, R={R:.2f}\")\n",
    "    \n",
    "    # Generate mean vectors (true signals)\n",
    "    mus = torch.randn(B, d)\n",
    "    mus = mus / torch.norm(mus, dim=1, keepdim=True) * R\n",
    "    \n",
    "    # Generate sequences with these means\n",
    "    y = (torch.rand(B, N) > 0.5).float()\n",
    "    y_signal = 2 * y - 1  # Convert to 1\n",
    "    \n",
    "    # Add noise to create inputs\n",
    "    z = torch.randn(B, N, d)  # Standard normal noise\n",
    "    x = y_signal.unsqueeze(-1) * mus.unsqueeze(1) + z\n",
    "    \n",
    "    print(\"\\nVerifying signal and noise properties:\")\n",
    "    signal_norms = torch.norm(y_signal.unsqueeze(-1) * mus.unsqueeze(1), dim=-1)\n",
    "    noise_norms = torch.norm(z, dim=-1)\n",
    "    print(f\"Signal norms: mean={signal_norms.mean():.3f}, std={signal_norms.std():.3f}\")\n",
    "    print(f\"Noise norms: mean={noise_norms.mean():.3f}, std={noise_norms.std():.3f}\")\n",
    "    \n",
    "    # Test prediction at each k\n",
    "    for k in range(1, 5):  # Just test first few k values\n",
    "        print(f\"\\nk = {k}\")\n",
    "        \n",
    "        # Get current context\n",
    "        context_x = x[:, :k]  # (B, k, d)\n",
    "        context_y = y[:, :k]  # (B, k)\n",
    "        next_x = x[:, k]      # (B, d)\n",
    "        next_y = y[:, k]      # (B)\n",
    "        \n",
    "        # Manual prediction computation\n",
    "        y_signal = 2 * context_y - 1\n",
    "        context_term = (1/k) * torch.sum(y_signal.unsqueeze(-1) * context_x, dim=1)\n",
    "        \n",
    "        # Since W is identity, just compute dot product with next_x\n",
    "        logits = torch.sum(context_term * next_x, dim=1)\n",
    "        preds = (logits > 0).float()\n",
    "        acc = (preds == next_y).float().mean().item()\n",
    "        \n",
    "        # Compute some diagnostics\n",
    "        signal_norm = torch.norm(context_term, dim=1)\n",
    "        print(f\"Accuracy: {acc:.3f}\")\n",
    "        print(f\"Signal norm: mean={signal_norm.mean():.3f}, std={signal_norm.std():.3f}\")\n",
    "        \n",
    "        # Look at a few example predictions in detail\n",
    "        for i in range(3):\n",
    "            alignment = torch.dot(context_term[i], mus[i]) / (torch.norm(context_term[i]) * torch.norm(mus[i]))\n",
    "            print(f\"\\nExample {i}:\")\n",
    "            print(f\"True y: {next_y[i].item()}\")\n",
    "            print(f\"Pred y: {preds[i].item()}\")\n",
    "            print(f\"Logit: {logits[i].item():.3f}\")\n",
    "            print(f\"Signal norm: {signal_norm[i].item():.3f}\")\n",
    "            print(f\"Alignment with true mu: {alignment.item():.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_identity_simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_icl import ExperimentConfig, LinearTransformer, GaussianMixtureDataset\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from dataclasses import asdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "class CheckpointEvaluator:\n",
    "    \"\"\"Evaluator class for analyzing trained model checkpoints\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir: str):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        \n",
    "    def load_checkpoint(self, checkpoint_path: str) -> Tuple[LinearTransformer, ExperimentConfig]:\n",
    "        \"\"\"Load model and config from checkpoint\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        \n",
    "        # Recreate config\n",
    "        config = ExperimentConfig(**asdict(checkpoint['config']))\n",
    "        \n",
    "        # Initialize and load model\n",
    "        model = LinearTransformer(config.d)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        # model.to(config.device)\n",
    "        model.eval()\n",
    "        \n",
    "        return model, config\n",
    "\n",
    "    def evaluate_sequential_predictions(\n",
    "        self, \n",
    "        model: LinearTransformer,\n",
    "        config: ExperimentConfig,\n",
    "        num_samples: int = 100,\n",
    "        max_seq_length: int = 50\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate model's sequential prediction performance.\n",
    "        \"\"\"\n",
    "        eval_dataset = GaussianMixtureDataset(\n",
    "            d=config.d,\n",
    "            N=max_seq_length,\n",
    "            B=num_samples,\n",
    "            R=config.R_val,\n",
    "            device=config.device,\n",
    "            is_validation=True,\n",
    "            label_flip_p=config.label_flip_p\n",
    "        )\n",
    "        \n",
    "        accuracies = []\n",
    "        confidences = []\n",
    "        position_accuracies = np.zeros(max_seq_length)\n",
    "        position_counts = np.zeros(max_seq_length)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            context_x, context_y, target_x, target_y = eval_dataset[0]\n",
    "            print(f\"Data shapes: context_x {context_x.shape}, context_y {context_y.shape}\")\n",
    "            print(f\"Data ranges: context_x [{context_x.min():.3f}, {context_x.max():.3f}], context_y [{context_y.min():.3f}, {context_y.max():.3f}]\")\n",
    "            \n",
    "            for i in range(num_samples):\n",
    "                x_seq = context_x[i]  # Shape: (N, d)\n",
    "                y_seq = context_y[i]  # Shape: (N,)\n",
    "                \n",
    "                seq_preds = []\n",
    "                seq_confs = []\n",
    "                \n",
    "                for k in range(max_seq_length - 1):\n",
    "                    curr_context_x = x_seq[:k+1][None, ...]  \n",
    "                    curr_context_y = y_seq[:k+1][None, ...]  \n",
    "                    curr_target_x = x_seq[k+1][None, ...]    \n",
    "                    curr_target_y = y_seq[k+1]              \n",
    "                    \n",
    "                    logits = model(curr_context_x, curr_context_y, curr_target_x)\n",
    "                    \n",
    "                    if torch.isnan(logits).any():\n",
    "                        print(f\"NaN in logits at position {k}, sequence {i}\")\n",
    "                        print(f\"Input ranges: context_x [{curr_context_x.min():.3f}, {curr_context_x.max():.3f}]\")\n",
    "                        continue\n",
    "                    \n",
    "                    pred = (logits > 0).float().item()\n",
    "                    conf = torch.sigmoid(logits).item()\n",
    "                    \n",
    "                    seq_preds.append(pred)\n",
    "                    seq_confs.append(conf)\n",
    "                    \n",
    "                    position_accuracies[k] += float(pred == curr_target_y.item())\n",
    "                    position_counts[k] += 1\n",
    "                \n",
    "                accuracies.append(seq_preds)\n",
    "                confidences.append(seq_confs)\n",
    "                \n",
    "                if i % 10 == 0:\n",
    "                    print(f\"Processed sequence {i}, current mean acc: {np.mean(position_accuracies[position_counts > 0] / position_counts[position_counts > 0]):.3f}\")\n",
    "        \n",
    "        # Safe division for position accuracies\n",
    "        valid_positions = position_counts > 0\n",
    "        if valid_positions.any():\n",
    "            position_accuracies[valid_positions] = position_accuracies[valid_positions] / position_counts[valid_positions]\n",
    "        \n",
    "        mean_acc = float(np.mean(position_accuracies[valid_positions])) if valid_positions.any() else 0.0\n",
    "        \n",
    "        metrics = {\n",
    "            'position_accuracies': position_accuracies.tolist(),  # convert to list for JSON serialization\n",
    "            'mean_accuracy': mean_acc,\n",
    "            'accuracies': accuracies,\n",
    "            'confidences': confidences,\n",
    "            'config': asdict(config)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nFinal metrics:\")\n",
    "        print(f\"Mean accuracy: {mean_acc:.3f}\")\n",
    "        print(f\"Position accuracies range: [{min(position_accuracies[valid_positions]):.3f}, {max(position_accuracies[valid_positions]):.3f}]\")\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def old_evaluate_sequential_predictions(\n",
    "        self, \n",
    "        model: LinearTransformer,\n",
    "        config: ExperimentConfig,\n",
    "        num_samples: int = 100,\n",
    "        max_seq_length: int = 50\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate model's sequential prediction performance.\n",
    "        Uses broadcasting notation instead of squeeze/unsqueeze.\n",
    "        \"\"\"\n",
    "        # Create dataset with longer sequences\n",
    "        eval_dataset = GaussianMixtureDataset(\n",
    "            d=config.d,\n",
    "            N=max_seq_length,\n",
    "            B=num_samples,\n",
    "            R=config.R_val,\n",
    "            device='cpu',\n",
    "            is_validation=True,\n",
    "            label_flip_p=config.label_flip_p\n",
    "        )\n",
    "        \n",
    "        # Storage for metrics\n",
    "        accuracies = []\n",
    "        confidences = []\n",
    "        position_accuracies = np.zeros(max_seq_length)\n",
    "        position_counts = np.zeros(max_seq_length)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get data using dataset's __getitem__\n",
    "            x_all, y_all, target_x, target_y = eval_dataset[0]  # Changed this line\n",
    "        \n",
    "            # For each sequence\n",
    "            for i in range(num_samples):\n",
    "                x_seq = x_all[i]   # Shape: (N, d)\n",
    "                y_seq = y_all[i]   # Shape: (N,)\n",
    "                \n",
    "                # For each position k, predict k+1 using 1:k\n",
    "                seq_preds = []\n",
    "                seq_confs = []\n",
    "                \n",
    "                for k in range(max_seq_length - 1):\n",
    "                    # Get context up to position k\n",
    "                    context_x = x_seq[:k+1][None, :, :]  # Add batch dim: (1, k+1, d)\n",
    "                    context_y = y_seq[:k+1][None, :]     # Add batch dim: (1, k+1)\n",
    "                    target_x = x_seq[k+1][None, :]       # Add batch dim: (1, d)\n",
    "                    target_y = y_seq[k+1]                # Keep as scalar\n",
    "                    \n",
    "                    # Make prediction\n",
    "                    logits = model(context_x, context_y, target_x)  # Shape: (1,)\n",
    "                    \n",
    "                    # Store prediction and confidence\n",
    "                    pred = (logits > 0).float().item()\n",
    "                    conf = torch.sigmoid(logits).item()\n",
    "                    seq_preds.append(pred)\n",
    "                    seq_confs.append(conf)\n",
    "                    \n",
    "                    # Update position accuracies\n",
    "                    position_accuracies[k] += (pred == target_y.item())\n",
    "                    position_counts[k] += 1\n",
    "                \n",
    "                # Store sequence metrics\n",
    "                accuracies.append(seq_preds)\n",
    "                confidences.append(seq_confs)\n",
    "        \n",
    "        # Compute metrics\n",
    "        position_accuracies = position_accuracies / position_counts\n",
    "        \n",
    "        metrics = {\n",
    "            'position_accuracies': position_accuracies,\n",
    "            'mean_accuracy': np.mean(position_accuracies),\n",
    "            'accuracies': accuracies,\n",
    "            'confidences': confidences,\n",
    "            'config': asdict(config)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def evaluate_all_checkpoints(\n",
    "        self,\n",
    "        checkpoint_pattern: str = \"checkpoint_step_*.pt\",\n",
    "        save_results: bool = True\n",
    "    ) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Evaluate all checkpoints matching pattern.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_pattern: Pattern to match checkpoint files\n",
    "            save_results: Whether to save results to disk\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping checkpoint paths to their metrics\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Find all checkpoint files\n",
    "        checkpoint_files = list(self.checkpoint_dir.glob(checkpoint_pattern))\n",
    "        print(f\"Found {len(checkpoint_files)} checkpoints to evaluate\")\n",
    "        \n",
    "        for checkpoint_file in checkpoint_files:\n",
    "            print(f\"\\nEvaluating {checkpoint_file}\")\n",
    "            \n",
    "            # Load checkpoint\n",
    "            model, config = self.load_checkpoint(checkpoint_file)\n",
    "            \n",
    "            # Run evaluation\n",
    "            metrics = self.evaluate_sequential_predictions(model, config)\n",
    "            \n",
    "            # Store results\n",
    "            results[str(checkpoint_file)] = metrics\n",
    "            \n",
    "        return results\n",
    "    \n",
    "\n",
    "    def analyze_results(self, results: Dict[str, Dict[str, Any]]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Analyze evaluation results across checkpoints.\n",
    "        \n",
    "        Args:\n",
    "            results: Dictionary of evaluation results per checkpoint\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with analysis\n",
    "        \"\"\"\n",
    "        records = []\n",
    "        \n",
    "        for checkpoint_path, metrics in results.items():\n",
    "            config = metrics['config']\n",
    "            \n",
    "            record = {\n",
    "                'checkpoint': checkpoint_path,\n",
    "                'd': config['d'],\n",
    "                'B': config['B'],\n",
    "                'R': config['R_train'],\n",
    "                'mean_accuracy': metrics['mean_accuracy'],\n",
    "                'final_accuracy': metrics['position_accuracies'][-1],\n",
    "                'early_accuracy': np.mean(metrics['position_accuracies'][:5]),\n",
    "            }\n",
    "            records.append(record)\n",
    "            \n",
    "        df = pd.DataFrame(records)\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "ckpt = CheckpointEvaluator('checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 checkpoints to evaluate\n",
      "\n",
      "Evaluating checkpoints/checkpoint_d50_B50_R13_20241118_163034_step_499.pt\n",
      "Data shapes: context_x torch.Size([100, 50, 50]), context_y torch.Size([100, 50])\n",
      "Data ranges: context_x [-9.566, 9.064], context_y [0.000, 1.000]\n",
      "Processed sequence 0, current mean acc: 1.000\n",
      "Processed sequence 10, current mean acc: 1.000\n",
      "Processed sequence 20, current mean acc: 1.000\n",
      "Processed sequence 30, current mean acc: 1.000\n",
      "Processed sequence 40, current mean acc: 1.000\n",
      "Processed sequence 50, current mean acc: 1.000\n",
      "Processed sequence 60, current mean acc: 1.000\n",
      "Processed sequence 70, current mean acc: 1.000\n",
      "Processed sequence 80, current mean acc: 1.000\n",
      "Processed sequence 90, current mean acc: 1.000\n",
      "\n",
      "Final metrics:\n",
      "Mean accuracy: 1.000\n",
      "Position accuracies range: [1.000, 1.000]\n",
      "\n",
      "Evaluating checkpoints/checkpoint_d500_B500_R111_20241118_163035_step_499.pt\n",
      "Data shapes: context_x torch.Size([100, 50, 500]), context_y torch.Size([100, 50])\n",
      "Data ranges: context_x [-23.959, 25.398], context_y [0.000, 1.000]\n",
      "Processed sequence 0, current mean acc: 1.000\n",
      "Processed sequence 10, current mean acc: 1.000\n",
      "Processed sequence 20, current mean acc: 1.000\n",
      "Processed sequence 30, current mean acc: 1.000\n",
      "Processed sequence 40, current mean acc: 1.000\n",
      "Processed sequence 50, current mean acc: 1.000\n",
      "Processed sequence 60, current mean acc: 1.000\n",
      "Processed sequence 70, current mean acc: 1.000\n",
      "Processed sequence 80, current mean acc: 1.000\n",
      "Processed sequence 90, current mean acc: 1.000\n",
      "\n",
      "Final metrics:\n",
      "Mean accuracy: 1.000\n",
      "Position accuracies range: [1.000, 1.000]\n",
      "\n",
      "Evaluating checkpoints/checkpoint_d2500_B2500_R35_20241118_171451_step_299.pt\n",
      "Data shapes: context_x torch.Size([100, 50, 2500]), context_y torch.Size([100, 50])\n",
      "Data ranges: context_x [-6.598, 6.266], context_y [0.000, 1.000]\n",
      "Processed sequence 0, current mean acc: 1.000\n",
      "Processed sequence 10, current mean acc: 1.000\n",
      "Processed sequence 20, current mean acc: 1.000\n",
      "Processed sequence 30, current mean acc: 1.000\n",
      "Processed sequence 40, current mean acc: 1.000\n",
      "Processed sequence 50, current mean acc: 1.000\n",
      "Processed sequence 60, current mean acc: 1.000\n",
      "Processed sequence 70, current mean acc: 1.000\n",
      "Processed sequence 80, current mean acc: 1.000\n",
      "Processed sequence 90, current mean acc: 1.000\n",
      "\n",
      "Final metrics:\n",
      "Mean accuracy: 1.000\n",
      "Position accuracies range: [1.000, 1.000]\n",
      "\n",
      "Evaluating checkpoints/checkpoint_d50_B50_R35_20241118_163034_step_499.pt\n",
      "Data shapes: context_x torch.Size([100, 50, 50]), context_y torch.Size([100, 50])\n",
      "Data ranges: context_x [-21.158, 20.656], context_y [0.000, 1.000]\n",
      "Processed sequence 0, current mean acc: 1.000\n",
      "Processed sequence 10, current mean acc: 1.000\n",
      "Processed sequence 20, current mean acc: 1.000\n",
      "Processed sequence 30, current mean acc: 1.000\n",
      "Processed sequence 40, current mean acc: 1.000\n",
      "Processed sequence 50, current mean acc: 1.000\n",
      "Processed sequence 60, current mean acc: 1.000\n",
      "Processed sequence 70, current mean acc: 1.000\n",
      "Processed sequence 80, current mean acc: 1.000\n",
      "Processed sequence 90, current mean acc: 1.000\n",
      "\n",
      "Final metrics:\n",
      "Mean accuracy: 1.000\n",
      "Position accuracies range: [1.000, 1.000]\n",
      "\n",
      "Evaluating checkpoints/checkpoint_d500_B500_R23_20241118_163034_step_499.pt\n",
      "Data shapes: context_x torch.Size([100, 50, 500]), context_y torch.Size([100, 50])\n",
      "Data ranges: context_x [-7.055, 7.597], context_y [0.000, 1.000]\n",
      "Processed sequence 0, current mean acc: 1.000\n",
      "Processed sequence 10, current mean acc: 1.000\n",
      "Processed sequence 20, current mean acc: 1.000\n",
      "Processed sequence 30, current mean acc: 1.000\n",
      "Processed sequence 40, current mean acc: 1.000\n",
      "Processed sequence 50, current mean acc: 1.000\n",
      "Processed sequence 60, current mean acc: 1.000\n",
      "Processed sequence 70, current mean acc: 1.000\n",
      "Processed sequence 80, current mean acc: 1.000\n",
      "Processed sequence 90, current mean acc: 1.000\n",
      "\n",
      "Final metrics:\n",
      "Mean accuracy: 1.000\n",
      "Position accuracies range: [1.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "eval = CheckpointEvaluator('checkpoints/')\n",
    "res = eval.evaluate_all_checkpoints(checkpoint_pattern = 'checkpoint*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eval.analyze_results(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned ON\n"
     ]
    }
   ],
   "source": [
    "pprint(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checkpoint</th>\n",
       "      <th>d</th>\n",
       "      <th>B</th>\n",
       "      <th>R</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>final_accuracy</th>\n",
       "      <th>early_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>checkpoints/checkpoint_d50_B50_R13_20241118_16...</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>35.355339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>checkpoints/checkpoint_d500_B500_R111_20241118...</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>111.803399</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>checkpoints/checkpoint_d2500_B2500_R35_2024111...</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>checkpoints/checkpoint_d50_B50_R35_20241118_16...</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>35.355339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>checkpoints/checkpoint_d500_B500_R23_20241118_...</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>111.803399</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          checkpoint     d     B           R  \\\n",
       "0  checkpoints/checkpoint_d50_B50_R13_20241118_16...    50    50   35.355339   \n",
       "1  checkpoints/checkpoint_d500_B500_R111_20241118...   500   500  111.803399   \n",
       "2  checkpoints/checkpoint_d2500_B2500_R35_2024111...  2500  2500  250.000000   \n",
       "3  checkpoints/checkpoint_d50_B50_R35_20241118_16...    50    50   35.355339   \n",
       "4  checkpoints/checkpoint_d500_B500_R23_20241118_...   500   500  111.803399   \n",
       "\n",
       "   mean_accuracy  final_accuracy  early_accuracy  \n",
       "0            1.0             0.0             1.0  \n",
       "1            1.0             0.0             1.0  \n",
       "2            1.0             0.0             1.0  \n",
       "3            1.0             0.0             1.0  \n",
       "4            1.0             0.0             1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ckpt = CheckpointEvaluator('checkpoints/')\n",
    "c = torch.load('checkpoints/checkpoint_d2500_B2500_R35_20241118_171451_step_299.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d': 2500,\n",
       " 'N': 40,\n",
       " 'B': 2500,\n",
       " 'B_val': 500,\n",
       " 'R_train': 250.0,\n",
       " 'R_val': 35.35533905932738,\n",
       " 'max_steps': 300,\n",
       " 'checkpoint_steps': [299],\n",
       " 'label_flip_p': 0.0,\n",
       " 'learning_rate': 0.01,\n",
       " 'use_cuda': True,\n",
       " 'use_wandb': False,\n",
       " 'wandb_project': 'linear-transformer',\n",
       " 'save_checkpoints': True,\n",
       " 'save_results': True,\n",
       " 'checkpoint_dir': 'checkpoints',\n",
       " 'results_dir': 'results_20241118_171451',\n",
       " 'experiment_name': 'd2500_B2500_R35_20241118_171451'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "asdict(c['config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config': ExperimentConfig(d=2500,\n",
      "                            N=40,\n",
      "                            B=2500,\n",
      "                            B_val=500,\n",
      "                            R_train=250.0,\n",
      "                            R_val=35.35533905932738,\n",
      "                            max_steps=300,\n",
      "                            checkpoint_steps=[299],\n",
      "                            label_flip_p=0.0,\n",
      "                            learning_rate=0.01,\n",
      "                            use_cuda=True,\n",
      "                            use_wandb=False,\n",
      "                            wandb_project='linear-transformer',\n",
      "                            save_checkpoints=True,\n",
      "                            save_results=True,\n",
      "                            checkpoint_dir='checkpoints',\n",
      "                            results_dir='results_20241118_171451',\n",
      "                            experiment_name='d2500_B2500_R35_20241118_171451'),\n",
      " 'metrics': {'batch_time': [1.220353603363037,\n",
      "                            0.5143530368804932,\n",
      "                            0.4907188415527344,\n",
      "                            0.4963827133178711,\n",
      "                            0.4868476390838623,\n",
      "                            0.47902917861938477,\n",
      "                            0.4860103130340576,\n",
      "                            0.48700499534606934,\n",
      "                            0.5128512382507324,\n",
      "                            0.5104348659515381,\n",
      "                            0.5379688739776611,\n",
      "                            0.5271506309509277,\n",
      "                            0.5213208198547363,\n",
      "                            0.5138890743255615,\n",
      "                            0.5324239730834961,\n",
      "                            0.5153262615203857,\n",
      "                            0.5211248397827148,\n",
      "                            0.5100245475769043,\n",
      "                            0.5251955986022949,\n",
      "                            0.495837926864624,\n",
      "                            0.5202724933624268,\n",
      "                            0.5314302444458008,\n",
      "                            0.49822211265563965,\n",
      "                            0.5099718570709229,\n",
      "                            0.5877134799957275,\n",
      "                            0.5169773101806641,\n",
      "                            0.5027217864990234,\n",
      "                            0.5114274024963379,\n",
      "                            0.48694801330566406,\n",
      "                            0.4866952896118164,\n",
      "                            0.5156276226043701,\n",
      "                            0.508018970489502,\n",
      "                            0.49681973457336426,\n",
      "                            0.5382323265075684,\n",
      "                            0.5175058841705322,\n",
      "                            0.5291769504547119,\n",
      "                            0.500216007232666,\n",
      "                            0.5384507179260254,\n",
      "                            0.5000400543212891,\n",
      "                            0.48374128341674805,\n",
      "                            0.4905576705932617,\n",
      "                            0.5203564167022705,\n",
      "                            0.5773928165435791,\n",
      "                            0.5068726539611816,\n",
      "                            0.5121479034423828,\n",
      "                            0.5138428211212158,\n",
      "                            0.5201318264007568,\n",
      "                            0.505882978439331,\n",
      "                            0.5065422058105469,\n",
      "                            0.48505687713623047,\n",
      "                            0.49062442779541016,\n",
      "                            0.5042049884796143,\n",
      "                            0.5097923278808594,\n",
      "                            0.5191256999969482,\n",
      "                            0.530095100402832,\n",
      "                            0.5261397361755371,\n",
      "                            0.49804258346557617,\n",
      "                            0.5095267295837402,\n",
      "                            0.5194380283355713,\n",
      "                            0.4986879825592041,\n",
      "                            0.5174784660339355,\n",
      "                            0.5114729404449463,\n",
      "                            0.5025796890258789,\n",
      "                            0.520195484161377,\n",
      "                            0.5154469013214111,\n",
      "                            0.5004000663757324,\n",
      "                            0.5262801647186279,\n",
      "                            0.5278520584106445,\n",
      "                            0.5273218154907227,\n",
      "                            0.4887242317199707,\n",
      "                            0.4967799186706543,\n",
      "                            0.4998905658721924,\n",
      "                            0.507333517074585,\n",
      "                            0.48929691314697266,\n",
      "                            0.49872517585754395,\n",
      "                            0.5302340984344482,\n",
      "                            0.5262308120727539,\n",
      "                            0.5261130332946777,\n",
      "                            0.5256803035736084,\n",
      "                            0.5008823871612549,\n",
      "                            0.5072677135467529,\n",
      "                            0.5187771320343018,\n",
      "                            0.5015203952789307,\n",
      "                            0.4929800033569336,\n",
      "                            0.4911341667175293,\n",
      "                            0.49663305282592773,\n",
      "                            0.5060510635375977,\n",
      "                            0.5024302005767822,\n",
      "                            0.49723076820373535,\n",
      "                            0.4938993453979492,\n",
      "                            0.4901454448699951,\n",
      "                            0.5188801288604736,\n",
      "                            0.494610071182251,\n",
      "                            0.4950075149536133,\n",
      "                            0.4929313659667969,\n",
      "                            0.5093510150909424,\n",
      "                            0.5366151332855225,\n",
      "                            0.5092403888702393,\n",
      "                            0.5041177272796631,\n",
      "                            0.5321786403656006,\n",
      "                            0.49962496757507324,\n",
      "                            0.5146903991699219,\n",
      "                            0.513493537902832,\n",
      "                            0.4922807216644287,\n",
      "                            0.5175364017486572,\n",
      "                            0.519972562789917,\n",
      "                            0.499896764755249,\n",
      "                            0.5111804008483887,\n",
      "                            0.51383376121521,\n",
      "                            0.512352466583252,\n",
      "                            0.5204322338104248,\n",
      "                            0.5053048133850098,\n",
      "                            0.5039551258087158,\n",
      "                            0.49833154678344727,\n",
      "                            0.5025715827941895,\n",
      "                            0.4995112419128418,\n",
      "                            0.4948995113372803,\n",
      "                            0.515439510345459,\n",
      "                            0.5110106468200684,\n",
      "                            0.5256004333496094,\n",
      "                            0.4889373779296875,\n",
      "                            0.504188060760498,\n",
      "                            0.49492383003234863,\n",
      "                            0.49561500549316406,\n",
      "                            0.5083489418029785,\n",
      "                            0.5196833610534668,\n",
      "                            0.5193171501159668,\n",
      "                            0.5140442848205566,\n",
      "                            0.49759340286254883,\n",
      "                            0.4987962245941162,\n",
      "                            0.5033473968505859,\n",
      "                            0.5060117244720459,\n",
      "                            0.5363163948059082,\n",
      "                            0.4874281883239746,\n",
      "                            0.5481679439544678,\n",
      "                            0.5426616668701172,\n",
      "                            0.5458788871765137,\n",
      "                            0.5164768695831299,\n",
      "                            0.5109198093414307,\n",
      "                            0.5265357494354248,\n",
      "                            0.525984525680542,\n",
      "                            0.5034193992614746,\n",
      "                            0.5596568584442139,\n",
      "                            0.513009786605835,\n",
      "                            0.5301914215087891,\n",
      "                            0.5235884189605713,\n",
      "                            0.5156309604644775,\n",
      "                            0.4888162612915039,\n",
      "                            0.5067226886749268,\n",
      "                            0.5216376781463623,\n",
      "                            0.5015461444854736,\n",
      "                            0.5181288719177246,\n",
      "                            0.48439908027648926,\n",
      "                            0.4906134605407715,\n",
      "                            0.4863736629486084,\n",
      "                            0.5179510116577148,\n",
      "                            0.5217132568359375,\n",
      "                            0.5163662433624268,\n",
      "                            0.6746258735656738,\n",
      "                            0.6850447654724121,\n",
      "                            0.6852231025695801,\n",
      "                            0.7159404754638672,\n",
      "                            0.657360315322876,\n",
      "                            0.6710283756256104,\n",
      "                            0.4888288974761963,\n",
      "                            0.6160781383514404,\n",
      "                            0.611088752746582,\n",
      "                            0.624300479888916,\n",
      "                            0.6554043292999268,\n",
      "                            0.644730806350708,\n",
      "                            0.6695680618286133,\n",
      "                            0.763660192489624,\n",
      "                            0.7307829856872559,\n",
      "                            0.6806285381317139,\n",
      "                            0.7620348930358887,\n",
      "                            0.7715373039245605,\n",
      "                            0.7257659435272217,\n",
      "                            0.6917216777801514,\n",
      "                            0.7628698348999023,\n",
      "                            0.7346453666687012,\n",
      "                            0.7698183059692383,\n",
      "                            0.7786655426025391,\n",
      "                            0.7636499404907227,\n",
      "                            0.7627623081207275,\n",
      "                            0.7646045684814453,\n",
      "                            0.7542812824249268,\n",
      "                            0.7857215404510498,\n",
      "                            0.7457654476165771,\n",
      "                            0.7803974151611328,\n",
      "                            0.7785289287567139,\n",
      "                            0.7537040710449219,\n",
      "                            0.7393903732299805,\n",
      "                            0.7434220314025879,\n",
      "                            0.7552430629730225,\n",
      "                            0.7753713130950928,\n",
      "                            0.7473673820495605,\n",
      "                            0.7748992443084717,\n",
      "                            0.7552645206451416,\n",
      "                            0.7328753471374512,\n",
      "                            0.7209987640380859,\n",
      "                            0.7293014526367188,\n",
      "                            0.745814323425293,\n",
      "                            0.634821891784668,\n",
      "                            0.791886568069458,\n",
      "                            0.7478880882263184,\n",
      "                            0.8554539680480957,\n",
      "                            0.8063516616821289,\n",
      "                            0.8158562183380127,\n",
      "                            0.7195539474487305,\n",
      "                            0.6949374675750732,\n",
      "                            0.8739149570465088,\n",
      "                            0.850567102432251,\n",
      "                            0.8858704566955566,\n",
      "                            0.7448627948760986,\n",
      "                            0.7382180690765381,\n",
      "                            0.7906768321990967,\n",
      "                            0.824298620223999,\n",
      "                            0.6872785091400146,\n",
      "                            0.7050116062164307,\n",
      "                            0.6749510765075684,\n",
      "                            0.7301590442657471,\n",
      "                            0.6988217830657959,\n",
      "                            0.8220436573028564,\n",
      "                            0.8220736980438232,\n",
      "                            0.8614206314086914,\n",
      "                            0.8403394222259521,\n",
      "                            0.798457145690918,\n",
      "                            0.8630468845367432,\n",
      "                            0.7174487113952637,\n",
      "                            0.7473735809326172,\n",
      "                            0.7919752597808838,\n",
      "                            0.84464430809021,\n",
      "                            0.5692172050476074,\n",
      "                            0.5133082866668701,\n",
      "                            0.529982328414917,\n",
      "                            0.5153474807739258,\n",
      "                            0.5138964653015137,\n",
      "                            0.5202033519744873,\n",
      "                            0.48617124557495117,\n",
      "                            0.5181570053100586,\n",
      "                            0.5202713012695312,\n",
      "                            0.5294971466064453,\n",
      "                            0.5063223838806152,\n",
      "                            0.4978156089782715,\n",
      "                            0.5109868049621582,\n",
      "                            0.5406448841094971,\n",
      "                            0.49948596954345703,\n",
      "                            0.5052580833435059,\n",
      "                            0.5388283729553223,\n",
      "                            0.5025923252105713,\n",
      "                            0.536309003829956,\n",
      "                            0.5014233589172363,\n",
      "                            0.525547981262207,\n",
      "                            0.5000638961791992,\n",
      "                            0.556290864944458,\n",
      "                            0.5093622207641602,\n",
      "                            0.5053343772888184,\n",
      "                            0.5072958469390869,\n",
      "                            0.5243594646453857,\n",
      "                            0.5150678157806396,\n",
      "                            0.4970569610595703,\n",
      "                            0.49314451217651367,\n",
      "                            0.5045089721679688,\n",
      "                            0.5216145515441895,\n",
      "                            0.542431116104126,\n",
      "                            0.5062477588653564,\n",
      "                            0.5300226211547852,\n",
      "                            0.503849983215332,\n",
      "                            0.5045220851898193,\n",
      "                            0.5211188793182373,\n",
      "                            0.5428407192230225,\n",
      "                            0.5182740688323975,\n",
      "                            0.5090069770812988,\n",
      "                            0.5093047618865967,\n",
      "                            0.4976673126220703,\n",
      "                            0.502979040145874,\n",
      "                            0.509955883026123,\n",
      "                            0.5232267379760742,\n",
      "                            0.520329475402832,\n",
      "                            0.5261504650115967,\n",
      "                            0.5283946990966797,\n",
      "                            0.5316026210784912,\n",
      "                            0.5476033687591553,\n",
      "                            0.5011773109436035,\n",
      "                            0.5145502090454102,\n",
      "                            0.5170562267303467,\n",
      "                            0.5003221035003662,\n",
      "                            0.49802327156066895,\n",
      "                            0.5087854862213135,\n",
      "                            0.5128564834594727,\n",
      "                            0.5215640068054199,\n",
      "                            0.5179898738861084,\n",
      "                            0.5133147239685059,\n",
      "                            0.5204379558563232,\n",
      "                            0.49854183197021484,\n",
      "                            0.5101830959320068,\n",
      "                            0.5137801170349121,\n",
      "                            0.4978299140930176,\n",
      "                            0.5567023754119873,\n",
      "                            0.5141584873199463],\n",
      "             'in_context_acc': [1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0,\n",
      "                                1.0],\n",
      "             'samples_per_second': [1402.9253388752338,\n",
      "                                    2051.228882652292,\n",
      "                                    2497.48205682652,\n",
      "                                    2819.1118975912523,\n",
      "                                    3062.3808941375587,\n",
      "                                    3254.0173029177436,\n",
      "                                    3402.3238790550517,\n",
      "                                    3522.141508364081,\n",
      "                                    3606.3899842066357,\n",
      "                                    3625.294082178717,\n",
      "                                    3628.2565319690766,\n",
      "                                    3635.4743883762007,\n",
      "                                    3643.379894191547,\n",
      "                                    3680.8804868449715,\n",
      "                                    3681.9968495487465,\n",
      "                                    3687.6809829727035,\n",
      "                                    3691.2367245107976,\n",
      "                                    3698.3355835315624,\n",
      "                                    3699.6907129349856,\n",
      "                                    3736.2488571149624,\n",
      "                                    3763.329897358615,\n",
      "                                    3761.3358138164963,\n",
      "                                    3790.861707589827,\n",
      "                                    3815.337351043358,\n",
      "                                    3797.5350470963795,\n",
      "                                    3798.5706616552184,\n",
      "                                    3821.449219273885,\n",
      "                                    3821.567719993392,\n",
      "                                    3845.6972224991146,\n",
      "                                    3868.1030054475987,\n",
      "                                    3884.0370936386153,\n",
      "                                    3900.286916262288,\n",
      "                                    3918.1470143495662,\n",
      "                                    3910.0866123025357,\n",
      "                                    3906.694033560016,\n",
      "                                    3900.784688488921,\n",
      "                                    3914.988806190793,\n",
      "                                    3907.4541698544876,\n",
      "                                    3921.31398486235,\n",
      "                                    3937.692975452714,\n",
      "                                    3946.0760658298236,\n",
      "                                    3941.1587562970676,\n",
      "                                    3929.051435015234,\n",
      "                                    3939.989699758234,\n",
      "                                    3950.241340933823,\n",
      "                                    3946.9881911032917,\n",
      "                                    3943.033470595097,\n",
      "                                    3940.707456045717,\n",
      "                                    3950.8946688153146,\n",
      "                                    3955.1343669785147,\n",
      "                                    3966.636105207739,\n",
      "                                    3976.1241287454172,\n",
      "                                    3972.9753313418078,\n",
      "                                    3968.7822691201623,\n",
      "                                    3963.550799866886,\n",
      "                                    3958.834850137349,\n",
      "                                    3968.28309829389,\n",
      "                                    3969.508411692639,\n",
      "                                    3966.238636624958,\n",
      "                                    3975.0300531811754,\n",
      "                                    3971.646467276939,\n",
      "                                    3974.0421983754045,\n",
      "                                    3981.9279126993056,\n",
      "                                    3978.2396861041652,\n",
      "                                    3975.1344600657726,\n",
      "                                    3982.805077814599,\n",
      "                                    3978.4808663462945,\n",
      "                                    3976.5804208304485,\n",
      "                                    3972.6009699226997,\n",
      "                                    3980.984082608943,\n",
      "                                    3988.463403446631,\n",
      "                                    3995.473458580855,\n",
      "                                    3993.1275639497835,\n",
      "                                    4000.811189732498,\n",
      "                                    4003.3647868903568,\n",
      "                                    3998.9631730997867,\n",
      "                                    3995.050552755437,\n",
      "                                    3990.9446187620556,\n",
      "                                    3987.2883688588536,\n",
      "                                    3993.533553393075,\n",
      "                                    3991.442136700494,\n",
      "                                    3996.1111742893495,\n",
      "                                    3998.097839842247,\n",
      "                                    4004.564328692207,\n",
      "                                    4011.038355145647,\n",
      "                                    4016.9505870396038,\n",
      "                                    4022.0537508162333,\n",
      "                                    4027.298924751851,\n",
      "                                    4025.798287081015,\n",
      "                                    4031.5052596981654,\n",
      "                                    4037.3920448195104,\n",
      "                                    4034.2073242583365,\n",
      "                                    4039.5724932120897,\n",
      "                                    4044.873099852992,\n",
      "                                    4050.210933171303,\n",
      "                                    4054.313667627817,\n",
      "                                    4049.685908661941,\n",
      "                                    4048.036527469324,\n",
      "                                    4045.9888700234533,\n",
      "                                    4042.862868770807,\n",
      "                                    4047.4532589404616,\n",
      "                                    4046.398303698077,\n",
      "                                    4049.9430790540428,\n",
      "                                    4054.828790846142,\n",
      "                                    4054.2045745876326,\n",
      "                                    4051.1678914601152,\n",
      "                                    4055.4404873325443,\n",
      "                                    4058.95331590404,\n",
      "                                    4056.272644186161,\n",
      "                                    4059.627439266466,\n",
      "                                    4056.6592563143245,\n",
      "                                    4060.127600154846,\n",
      "                                    4063.8668776029535,\n",
      "                                    4067.873921852396,\n",
      "                                    4067.172492005146,\n",
      "                                    4071.027707907994,\n",
      "                                    4075.079151523819,\n",
      "                                    4077.9066516329553,\n",
      "                                    4080.9432697853194,\n",
      "                                    4077.7415670123455,\n",
      "                                    4081.9425993294167,\n",
      "                                    4085.255654672898,\n",
      "                                    4089.0086885743144,\n",
      "                                    4092.696820805252,\n",
      "                                    4095.6282705390954,\n",
      "                                    4093.4470144585825,\n",
      "                                    4095.744545066791,\n",
      "                                    4093.2080169156834,\n",
      "                                    4096.606626263438,\n",
      "                                    4099.896120935429,\n",
      "                                    4102.907788956715,\n",
      "                                    4105.724998690969,\n",
      "                                    4101.979181619896,\n",
      "                                    4105.699379664406,\n",
      "                                    4101.476931430492,\n",
      "                                    4098.675016448293,\n",
      "                                    4096.366269245658,\n",
      "                                    4093.8092998349057,\n",
      "                                    4093.0881377138994,\n",
      "                                    4090.612318887153,\n",
      "                                    4087.745621246432,\n",
      "                                    4090.590533851272,\n",
      "                                    4088.8898463343253,\n",
      "                                    4086.666793132038,\n",
      "                                    4085.0448714890463,\n",
      "                                    4086.854962218105,\n",
      "                                    4086.0606076254194,\n",
      "                                    4089.450090728707,\n",
      "                                    4092.007171221967,\n",
      "                                    4090.0544348543835,\n",
      "                                    4092.789093879001,\n",
      "                                    4090.722965272248,\n",
      "                                    4094.1370359438697,\n",
      "                                    4097.018559717621,\n",
      "                                    4100.30593089995,\n",
      "                                    4102.201827384087,\n",
      "                                    4103.3047543333,\n",
      "                                    4105.2115759413255,\n",
      "                                    4099.527120171597,\n",
      "                                    4092.70236701786,\n",
      "                                    4086.685151981674,\n",
      "                                    4079.7976196990016,\n",
      "                                    4074.5891122315147,\n",
      "                                    4068.6413908359177,\n",
      "                                    4071.7854658527463,\n",
      "                                    4066.7491298223135,\n",
      "                                    4063.9961067075096,\n",
      "                                    4060.7005701414505,\n",
      "                                    4056.3008400428475,\n",
      "                                    4052.196703370171,\n",
      "                                    4047.482623492037,\n",
      "                                    4039.1013882573266,\n",
      "                                    4032.2357651964303,\n",
      "                                    4027.2441545048428,\n",
      "                                    4019.4974610927593,\n",
      "                                    4011.2986634744548,\n",
      "                                    4004.5074421884233,\n",
      "                                    3999.590180631238,\n",
      "                                    3992.139937151614,\n",
      "                                    3986.026624022603,\n",
      "                                    3978.8774621357047,\n",
      "                                    3971.2854837123764,\n",
      "                                    3963.63413844061,\n",
      "                                    3956.7801397371995,\n",
      "                                    3949.841108771478,\n",
      "                                    3943.42617777897,\n",
      "                                    3935.843773500919,\n",
      "                                    3929.596442584587,\n",
      "                                    3922.6868047638004,\n",
      "                                    3915.836516033095,\n",
      "                                    3909.952022668107,\n",
      "                                    3904.6238377009086,\n",
      "                                    3898.885487049238,\n",
      "                                    3893.173879684765,\n",
      "                                    3886.4819755433364,\n",
      "                                    3880.826940794659,\n",
      "                                    3874.3141954482867,\n",
      "                                    3868.3358505856004,\n",
      "                                    3863.6978157178005,\n",
      "                                    3859.286522935477,\n",
      "                                    3854.5042873230914,\n",
      "                                    3849.683692864068,\n",
      "                                    3848.1024876218175,\n",
      "                                    3841.9936011107843,\n",
      "                                    3836.4133386865365,\n",
      "                                    3827.704433551728,\n",
      "                                    3820.821409484945,\n",
      "                                    3813.930448133375,\n",
      "                                    3810.0656663889526,\n",
      "                                    3806.9043822594567,\n",
      "                                    3798.728248015784,\n",
      "                                    3791.1216205146643,\n",
      "                                    3782.454807199958,\n",
      "                                    3778.0209979700217,\n",
      "                                    3773.3950251710403,\n",
      "                                    3767.8856604147663,\n",
      "                                    3761.491897393462,\n",
      "                                    3758.9319983628593,\n",
      "                                    3756.0526663358605,\n",
      "                                    3753.7929017183374,\n",
      "                                    3750.0292213373764,\n",
      "                                    3747.0818061629316,\n",
      "                                    3741.2798157223865,\n",
      "                                    3735.4130078250614,\n",
      "                                    3728.8904738943716,\n",
      "                                    3722.434335347652,\n",
      "                                    3717.015760756965,\n",
      "                                    3710.5650721970983,\n",
      "                                    3707.4776199186003,\n",
      "                                    3703.3781395968635,\n",
      "                                    3698.6150025266734,\n",
      "                                    3692.5490572323006,\n",
      "                                    3693.0429613272777,\n",
      "                                    3695.6422649478995,\n",
      "                                    3696.228734848955,\n",
      "                                    3698.820970318038,\n",
      "                                    3699.6056567799574,\n",
      "                                    3699.9166259239264,\n",
      "                                    3703.134850964581,\n",
      "                                    3703.6118325224083,\n",
      "                                    3706.0531005977946,\n",
      "                                    3706.162181377309,\n",
      "                                    3708.855845076412,\n",
      "                                    3711.7589553821153,\n",
      "                                    3712.3268474950196,\n",
      "                                    3712.0112187457967,\n",
      "                                    3714.8294898597596,\n",
      "                                    3717.505280955021,\n",
      "                                    3719.415457476354,\n",
      "                                    3722.1137336162724,\n",
      "                                    3721.9486655774417,\n",
      "                                    3724.6415043600196,\n",
      "                                    3726.7896408768315,\n",
      "                                    3728.2728682195543,\n",
      "                                    3727.8835021459854,\n",
      "                                    3728.207412283457,\n",
      "                                    3730.706014211388,\n",
      "                                    3733.1855922554014,\n",
      "                                    3735.2908691864714,\n",
      "                                    3736.6342788423467,\n",
      "                                    3739.2954927982632,\n",
      "                                    3742.0223747176324,\n",
      "                                    3744.4931319342036,\n",
      "                                    3744.517702182919,\n",
      "                                    3744.078881806372,\n",
      "                                    3746.3773849123313,\n",
      "                                    3746.6092806191696,\n",
      "                                    3749.031990836385,\n",
      "                                    3751.433626883524,\n",
      "                                    3751.4309380793893,\n",
      "                                    3750.932867093044,\n",
      "                                    3752.8008193026517,\n",
      "                                    3755.05555403122,\n",
      "                                    3755.3217644814195,\n",
      "                                    3757.785917662401,\n",
      "                                    3760.1330153839417,\n",
      "                                    3762.322755871421,\n",
      "                                    3762.90074825674,\n",
      "                                    3764.4966847411624,\n",
      "                                    3764.349917226396,\n",
      "                                    3764.161893154959,\n",
      "                                    3763.956430169589,\n",
      "                                    3763.3606038344374,\n",
      "                                    3765.6650606933426,\n",
      "                                    3767.67322745681,\n",
      "                                    3767.686255781696,\n",
      "                                    3769.9746196528536,\n",
      "                                    3772.288436588324,\n",
      "                                    3774.3837664824555,\n",
      "                                    3774.5451343967625,\n",
      "                                    3776.3683519988635,\n",
      "                                    3776.563919134964,\n",
      "                                    3776.6018209419276,\n",
      "                                    3776.5108777745486,\n",
      "                                    3777.0315002945667,\n",
      "                                    3779.0415064094627,\n",
      "                                    3780.9641053044547,\n",
      "                                    3783.1887619858717,\n",
      "                                    3783.8781061187306,\n",
      "                                    3783.897156416674],\n",
      "             'step': [0,\n",
      "                      1,\n",
      "                      2,\n",
      "                      3,\n",
      "                      4,\n",
      "                      5,\n",
      "                      6,\n",
      "                      7,\n",
      "                      8,\n",
      "                      9,\n",
      "                      10,\n",
      "                      11,\n",
      "                      12,\n",
      "                      13,\n",
      "                      14,\n",
      "                      15,\n",
      "                      16,\n",
      "                      17,\n",
      "                      18,\n",
      "                      19,\n",
      "                      20,\n",
      "                      21,\n",
      "                      22,\n",
      "                      23,\n",
      "                      24,\n",
      "                      25,\n",
      "                      26,\n",
      "                      27,\n",
      "                      28,\n",
      "                      29,\n",
      "                      30,\n",
      "                      31,\n",
      "                      32,\n",
      "                      33,\n",
      "                      34,\n",
      "                      35,\n",
      "                      36,\n",
      "                      37,\n",
      "                      38,\n",
      "                      39,\n",
      "                      40,\n",
      "                      41,\n",
      "                      42,\n",
      "                      43,\n",
      "                      44,\n",
      "                      45,\n",
      "                      46,\n",
      "                      47,\n",
      "                      48,\n",
      "                      49,\n",
      "                      50,\n",
      "                      51,\n",
      "                      52,\n",
      "                      53,\n",
      "                      54,\n",
      "                      55,\n",
      "                      56,\n",
      "                      57,\n",
      "                      58,\n",
      "                      59,\n",
      "                      60,\n",
      "                      61,\n",
      "                      62,\n",
      "                      63,\n",
      "                      64,\n",
      "                      65,\n",
      "                      66,\n",
      "                      67,\n",
      "                      68,\n",
      "                      69,\n",
      "                      70,\n",
      "                      71,\n",
      "                      72,\n",
      "                      73,\n",
      "                      74,\n",
      "                      75,\n",
      "                      76,\n",
      "                      77,\n",
      "                      78,\n",
      "                      79,\n",
      "                      80,\n",
      "                      81,\n",
      "                      82,\n",
      "                      83,\n",
      "                      84,\n",
      "                      85,\n",
      "                      86,\n",
      "                      87,\n",
      "                      88,\n",
      "                      89,\n",
      "                      90,\n",
      "                      91,\n",
      "                      92,\n",
      "                      93,\n",
      "                      94,\n",
      "                      95,\n",
      "                      96,\n",
      "                      97,\n",
      "                      98,\n",
      "                      99,\n",
      "                      100,\n",
      "                      101,\n",
      "                      102,\n",
      "                      103,\n",
      "                      104,\n",
      "                      105,\n",
      "                      106,\n",
      "                      107,\n",
      "                      108,\n",
      "                      109,\n",
      "                      110,\n",
      "                      111,\n",
      "                      112,\n",
      "                      113,\n",
      "                      114,\n",
      "                      115,\n",
      "                      116,\n",
      "                      117,\n",
      "                      118,\n",
      "                      119,\n",
      "                      120,\n",
      "                      121,\n",
      "                      122,\n",
      "                      123,\n",
      "                      124,\n",
      "                      125,\n",
      "                      126,\n",
      "                      127,\n",
      "                      128,\n",
      "                      129,\n",
      "                      130,\n",
      "                      131,\n",
      "                      132,\n",
      "                      133,\n",
      "                      134,\n",
      "                      135,\n",
      "                      136,\n",
      "                      137,\n",
      "                      138,\n",
      "                      139,\n",
      "                      140,\n",
      "                      141,\n",
      "                      142,\n",
      "                      143,\n",
      "                      144,\n",
      "                      145,\n",
      "                      146,\n",
      "                      147,\n",
      "                      148,\n",
      "                      149,\n",
      "                      150,\n",
      "                      151,\n",
      "                      152,\n",
      "                      153,\n",
      "                      154,\n",
      "                      155,\n",
      "                      156,\n",
      "                      157,\n",
      "                      158,\n",
      "                      159,\n",
      "                      160,\n",
      "                      161,\n",
      "                      162,\n",
      "                      163,\n",
      "                      164,\n",
      "                      165,\n",
      "                      166,\n",
      "                      167,\n",
      "                      168,\n",
      "                      169,\n",
      "                      170,\n",
      "                      171,\n",
      "                      172,\n",
      "                      173,\n",
      "                      174,\n",
      "                      175,\n",
      "                      176,\n",
      "                      177,\n",
      "                      178,\n",
      "                      179,\n",
      "                      180,\n",
      "                      181,\n",
      "                      182,\n",
      "                      183,\n",
      "                      184,\n",
      "                      185,\n",
      "                      186,\n",
      "                      187,\n",
      "                      188,\n",
      "                      189,\n",
      "                      190,\n",
      "                      191,\n",
      "                      192,\n",
      "                      193,\n",
      "                      194,\n",
      "                      195,\n",
      "                      196,\n",
      "                      197,\n",
      "                      198,\n",
      "                      199,\n",
      "                      200,\n",
      "                      201,\n",
      "                      202,\n",
      "                      203,\n",
      "                      204,\n",
      "                      205,\n",
      "                      206,\n",
      "                      207,\n",
      "                      208,\n",
      "                      209,\n",
      "                      210,\n",
      "                      211,\n",
      "                      212,\n",
      "                      213,\n",
      "                      214,\n",
      "                      215,\n",
      "                      216,\n",
      "                      217,\n",
      "                      218,\n",
      "                      219,\n",
      "                      220,\n",
      "                      221,\n",
      "                      222,\n",
      "                      223,\n",
      "                      224,\n",
      "                      225,\n",
      "                      226,\n",
      "                      227,\n",
      "                      228,\n",
      "                      229,\n",
      "                      230,\n",
      "                      231,\n",
      "                      232,\n",
      "                      233,\n",
      "                      234,\n",
      "                      235,\n",
      "                      236,\n",
      "                      237,\n",
      "                      238,\n",
      "                      239,\n",
      "                      240,\n",
      "                      241,\n",
      "                      242,\n",
      "                      243,\n",
      "                      244,\n",
      "                      245,\n",
      "                      246,\n",
      "                      247,\n",
      "                      248,\n",
      "                      249,\n",
      "                      250,\n",
      "                      251,\n",
      "                      252,\n",
      "                      253,\n",
      "                      254,\n",
      "                      255,\n",
      "                      256,\n",
      "                      257,\n",
      "                      258,\n",
      "                      259,\n",
      "                      260,\n",
      "                      261,\n",
      "                      262,\n",
      "                      263,\n",
      "                      264,\n",
      "                      265,\n",
      "                      266,\n",
      "                      267,\n",
      "                      268,\n",
      "                      269,\n",
      "                      270,\n",
      "                      271,\n",
      "                      272,\n",
      "                      273,\n",
      "                      274,\n",
      "                      275,\n",
      "                      276,\n",
      "                      277,\n",
      "                      278,\n",
      "                      279,\n",
      "                      280,\n",
      "                      281,\n",
      "                      282,\n",
      "                      283,\n",
      "                      284,\n",
      "                      285,\n",
      "                      286,\n",
      "                      287,\n",
      "                      288,\n",
      "                      289,\n",
      "                      290,\n",
      "                      291,\n",
      "                      292,\n",
      "                      293,\n",
      "                      294,\n",
      "                      295,\n",
      "                      296,\n",
      "                      297,\n",
      "                      298,\n",
      "                      299],\n",
      "             'train_acc': [0.5015999674797058,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0,\n",
      "                           1.0],\n",
      "             'train_loss': [0.6931471228599548,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0],\n",
      "             'val_acc': [1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0,\n",
      "                         1.0],\n",
      "             'val_loss': [0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0,\n",
      "                          0.0]},\n",
      " 'model_state_dict': OrderedDict([('W',\n",
      "                                   tensor([[ 0.1181, -0.0016,  0.0037,  ..., -0.0002, -0.0005, -0.0025],\n",
      "        [-0.0006,  0.1175,  0.0020,  ...,  0.0013, -0.0031, -0.0021],\n",
      "        [ 0.0028,  0.0008,  0.1228,  ...,  0.0002,  0.0039,  0.0021],\n",
      "        ...,\n",
      "        [-0.0005,  0.0008, -0.0008,  ...,  0.1170,  0.0051, -0.0017],\n",
      "        [-0.0008, -0.0027,  0.0039,  ...,  0.0059,  0.1209,  0.0041],\n",
      "        [-0.0016, -0.0010,  0.0014,  ..., -0.0010,  0.0041,  0.1231]],\n",
      "       device='cuda:0'))]),\n",
      " 'optimizer_state_dict': {'param_groups': [{'dampening': 0,\n",
      "                                            'differentiable': False,\n",
      "                                            'foreach': None,\n",
      "                                            'lr': 0.01,\n",
      "                                            'maximize': False,\n",
      "                                            'momentum': 0.0,\n",
      "                                            'nesterov': False,\n",
      "                                            'params': [0],\n",
      "                                            'weight_decay': 0}],\n",
      "                          'state': {0: {'momentum_buffer': None}}},\n",
      " 'step': 299}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "pprint(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synforward",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
